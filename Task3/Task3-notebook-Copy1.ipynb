{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa07d14f",
   "metadata": {
    "cellUniqueIdByVincent": "46c01"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970143b",
   "metadata": {
    "cellUniqueIdByVincent": "4592b"
   },
   "outputs": [],
   "source": [
    "ct_file = \"../Task1/image/left_knee.nii.gz\"\n",
    "mask_file = \"..//Task1/output/bone_segmentation_task1_1.nii.gz\"\n",
    "output_folder = \"segmented_regions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d5ef",
   "metadata": {
    "cellUniqueIdByVincent": "eacd2"
   },
   "outputs": [],
   "source": [
    "def visualize(image_data,name):\n",
    "    # --- Choose slices to visualize ---\n",
    "    num_slices = 9\n",
    "    slice_indices = np.linspace(0, image_data.shape[2] - 1, num_slices, dtype=int)\n",
    "\n",
    "    # --- Plot slices ---\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        axes[i].imshow(image_data[:, :, slice_idx], cmap='gray')\n",
    "        axes[i].set_title(f\"Slice {slice_idx}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.savefig(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file and return the data array and affine.\n",
    "    \"\"\"\n",
    "    nifti = nib.load(file_path)\n",
    "    data = nifti.get_fdata()\n",
    "    affine = nifti.affine\n",
    "    return data, affine\n",
    "\n",
    "\n",
    "def save_nifti(data, affine, output_path):\n",
    "    \"\"\"\n",
    "    Save a NumPy array as a NIfTI file.\n",
    "    \"\"\"\n",
    "    nifti = nib.Nifti1Image(data.astype(np.float32), affine)\n",
    "    nib.save(nifti, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26efb2",
   "metadata": {
    "cellUniqueIdByVincent": "805c5"
   },
   "outputs": [],
   "source": [
    "image_data,_=load_nifti(ct_file)\n",
    "visualize(image_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b2669",
   "metadata": {
    "cellUniqueIdByVincent": "f8fff"
   },
   "outputs": [],
   "source": [
    "def segment_knee_regions(ct_path, mask_path, output_dir):\n",
    "    \"\"\"\n",
    "    Segment Tibia, Femur, and Background from a CT volume using the mask.\n",
    "    \"\"\"\n",
    "    # Load the CT scan and segmentation mask\n",
    "    ct_data, ct_affine = load_nifti(ct_path)\n",
    "    mask_data, _ = load_nifti(mask_path)\n",
    "\n",
    "    # Define region labels (change if your mask uses different values)\n",
    "    TIBIA_LABEL = 1\n",
    "    FEMUR_LABEL = 2\n",
    "    BACKGROUND_LABEL = 0\n",
    "\n",
    "    # Generate binary masks\n",
    "    tibia_mask = (mask_data == TIBIA_LABEL)\n",
    "    femur_mask = (mask_data == FEMUR_LABEL)\n",
    "    background_mask = (mask_data == BACKGROUND_LABEL)\n",
    "\n",
    "    # Apply masks to CT data\n",
    "    tibia_volume = ct_data * tibia_mask\n",
    "    femur_volume = ct_data * femur_mask\n",
    "    background_volume = ct_data * background_mask\n",
    "\n",
    "    # Save each region as a new .nii.gz file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_nifti(tibia_volume, ct_affine, os.path.join(output_dir, \"tibia_volume.nii.gz\"))\n",
    "    save_nifti(femur_volume, ct_affine, os.path.join(output_dir, \"femur_volume.nii.gz\"))\n",
    "    save_nifti(background_volume, ct_affine, os.path.join(output_dir, \"background_volume.nii.gz\"))\n",
    "\n",
    "    print(\"Segmentation done. Files saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209b078",
   "metadata": {
    "cellUniqueIdByVincent": "0d98f"
   },
   "outputs": [],
   "source": [
    "segment_knee_regions(ct_file, mask_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ead239",
   "metadata": {
    "cellUniqueIdByVincent": "6e590"
   },
   "outputs": [],
   "source": [
    "def overlay_mask(image_slice, mask_slice, alpha=0.4, cmap_img='gray', cmap_mask='jet'):\n",
    "    \"\"\"\n",
    "    Overlay a segmentation mask on a single image slice.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_slice: 2D numpy array of the image slice.\n",
    "    - mask_slice: 2D numpy array of the mask slice (same size as image_slice).\n",
    "    - alpha: float, transparency of the mask overlay.\n",
    "    - cmap_img: str, colormap for the image.\n",
    "    - cmap_mask: str, colormap for the mask.\n",
    "    \"\"\"\n",
    "    plt.imshow(image_slice, cmap=cmap_img)\n",
    "    if mask_slice is not None and np.any(mask_slice):\n",
    "        plt.imshow(mask_slice, cmap=cmap_mask, alpha=alpha)\n",
    "    plt.axis('off')\n",
    "\n",
    "def visualize_nifti_with_mask(image_path, mask_path=None, num_slices=9, axis=2):\n",
    "    \"\"\"\n",
    "    Visualize NIfTI image slices with optional mask overlays.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image .nii.gz file.\n",
    "    - mask_path: str or None, path to the mask .nii.gz file (optional).\n",
    "    - num_slices: int, number of slices to visualize.\n",
    "    - axis: int, axis along which to slice (0=sagittal, 1=coronal, 2=axial).\n",
    "    \"\"\"\n",
    "    # Load image and optional mask\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    assert mask is None or image.shape == mask.shape, \"Image and mask must have the same shape\"\n",
    "\n",
    "    # Select slices evenly along the chosen axis\n",
    "    slice_indices = np.linspace(0, image.shape[axis] - 1, num_slices, dtype=int)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(slice_indices):\n",
    "        ax = axes[i]\n",
    "        plt.sca(ax)\n",
    "\n",
    "        if axis == 0:\n",
    "            img_slice = image[idx, :, :]\n",
    "            msk_slice = mask[idx, :, :] if mask is not None else None\n",
    "        elif axis == 1:\n",
    "            img_slice = image[:, idx, :]\n",
    "            msk_slice = mask[:, idx, :] if mask is not None else None\n",
    "        else:  # default to axial\n",
    "            img_slice = image[:, :, idx]\n",
    "            msk_slice = mask[:, :, idx] if mask is not None else None\n",
    "\n",
    "        overlay_mask(img_slice, msk_slice)\n",
    "        ax.set_title(f\"Slice {idx}\")\n",
    "    plt.savefig('slices with masks ')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80cb7b",
   "metadata": {
    "cellUniqueIdByVincent": "6ea89"
   },
   "outputs": [],
   "source": [
    "visualize_nifti_with_mask(\n",
    "    ct_file,\n",
    "    mask_file,  # Set to None if you don’t have a mask\n",
    "    num_slices=9,\n",
    "    axis=2  # axial view\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f6bb2",
   "metadata": {
    "cellUniqueIdByVincent": "f16a9"
   },
   "outputs": [],
   "source": [
    "def plot_image_mask_overlay(image_path, mask_path, slice_index, axis=2, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Plot image slice, mask slice, and overlay side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: 3D numpy array (CT volume)\n",
    "    - mask: 3D numpy array (segmentation mask)\n",
    "    - slice_index: int, index of the slice to visualize\n",
    "    - axis: int, slicing axis (0=sagittal, 1=coronal, 2=axial)\n",
    "    - alpha: float, transparency for overlay\n",
    "    \"\"\"\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    # Extract slices\n",
    "    if axis == 0:\n",
    "        img_slice = image[slice_index, :, :]\n",
    "        mask_slice = mask[slice_index, :, :]\n",
    "    elif axis == 1:\n",
    "        img_slice = image[:, slice_index, :]\n",
    "        mask_slice = mask[:, slice_index, :]\n",
    "    else:\n",
    "        img_slice = image[:, :, slice_index]\n",
    "        mask_slice = mask[:, :, slice_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(img_slice, cmap='gray')\n",
    "    axs[0].set_title(f'Image Slice {slice_index}')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(mask_slice, cmap='jet')\n",
    "    axs[1].set_title(f'Mask Slice {slice_index}')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(img_slice, cmap='gray')\n",
    "    axs[2].imshow(mask_slice, cmap='jet', alpha=alpha)\n",
    "    axs[2].set_title('Overlay')\n",
    "    axs[2].axis('off')\n",
    "    plt.savefig('slice_108_with_maska')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b19e7",
   "metadata": {
    "cellUniqueIdByVincent": "0d7eb"
   },
   "outputs": [],
   "source": [
    "slice_index = 108  \n",
    "axis = 2  # Axial view\n",
    "\n",
    "plot_image_mask_overlay(ct_file, mask_file, slice_index, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34ea979",
   "metadata": {
    "cellUniqueIdByVincent": "cf54c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49713466",
   "metadata": {
    "cellUniqueIdByVincent": "6ab06"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf5d25a3",
   "metadata": {
    "cellUniqueIdByVincent": "ce7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2D DenseNet121 model.\n",
      "\n",
      "Inflated 3D DenseNet model created for 16 frames.\n",
      "\n",
      "Dummy input shape: torch.Size([2, 1, 16, 224, 224])\n",
      "\n",
      "Forward pass successful!\n",
      "Output shape: torch.Size([2, 2048])\n",
      "Expected output shape: (2, 1000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "# --- Helper Functions for Inflation ---\n",
    "\n",
    "def inflate_conv(conv2d, kernel_depth):\n",
    "    \"\"\"Inflates a Conv2d layer to a Conv3d layer by repeating weights.\"\"\"\n",
    "    if conv2d.in_channels % conv2d.groups != 0 or conv2d.out_channels % conv2d.groups != 0:\n",
    "         raise ValueError(\"Inflating grouped convolutions is not straightforward with simple repetition.\")\n",
    "\n",
    "    # Get 2D parameters\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size_2d = conv2d.kernel_size\n",
    "    stride_2d = conv2d.stride\n",
    "    padding_2d = conv2d.padding\n",
    "    dilation_2d = conv2d.dilation\n",
    "    groups = conv2d.groups\n",
    "    bias = conv2d.bias is not None\n",
    "\n",
    "    # Create 3D convolution parameters\n",
    "    # Temporal kernel size\n",
    "    kernel_size_3d = (kernel_depth, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    \n",
    "    # Typically stride 1 in time for simple inflation unless downsampling is desired\n",
    "    # Based on the previous I3D code structure, transitions downsample spatially,\n",
    "    # and the initial conv might downsample spatially. Temporal downsampling is handled\n",
    "    # by the pooling layers in transition blocks.\n",
    "    # So we'll use a temporal stride of 1 here for convolution inflation.\n",
    "    stride_3d = (1, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding to keep temporal dimension size\n",
    "    padding_3d = (kernel_depth // 2, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1]) # Dilation usually 1 in time\n",
    "\n",
    "    # Create 3D Conv layer\n",
    "    conv3d = nn.Conv3d(in_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                       dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "    # Inflate weights (replication and scaling)\n",
    "    conv2d_weights = conv2d.weight.data #  Shape [out_channels, 3, H, W]\n",
    "    #averaged_weights = conv2d_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "\n",
    "                 # Now repeat this 1-channel weight along the temporal dimension\n",
    "    inflated_weights = conv2d_weights.unsqueeze(2).repeat(1, 1, kernel_depth, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize by dividing by the depth (as per requirement ii)\n",
    "    if kernel_depth > 0: # Avoid division by zero if somehow kernel_depth is 0\n",
    "        inflated_weights = inflated_weights / kernel_depth\n",
    "\n",
    "    # Copy inflated weights to the 3D Conv layer\n",
    "    conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "    # Copy bias if it exists\n",
    "    if bias:\n",
    "        conv3d.bias.data.copy_(conv2d.bias.data)\n",
    "\n",
    "    return conv3d\n",
    "\n",
    "def inflate_batch_norm(bn2d):\n",
    "    \"\"\"Inflates a BatchNorm2d layer to a BatchNorm3d layer.\"\"\"\n",
    "    bn3d = nn.BatchNorm3d(bn2d.num_features)\n",
    "    # Copy parameters and running statistics\n",
    "    bn3d.weight.data.copy_(bn2d.weight.data)\n",
    "    bn3d.bias.data.copy_(bn2d.bias.data)\n",
    "    bn3d.running_mean.copy_(bn2d.running_mean)\n",
    "    bn3d.running_var.copy_(bn2d.running_var)\n",
    "    # bn3d.num_batches_tracked.copy_(bn2d.num_batches_tracked) # Copy if your BN uses this\n",
    "    return bn3d\n",
    "\n",
    "def inflate_relu(relu2d):\n",
    "    \"\"\"Returns a ReLU layer (same for 2D and 3D).\"\"\"\n",
    "    return nn.ReLU(inplace=relu2d.inplace)\n",
    "\n",
    "def inflate_pool(pool2d, temporal_stride=1):\n",
    "    \"\"\"Inflates a Pooling layer to a 3D Pooling layer.\"\"\"\n",
    "    # Get 2D parameters\n",
    "    kernel_size_2d = pool2d.kernel_size\n",
    "    stride_2d = pool2d.stride\n",
    "    padding_2d = pool2d.padding\n",
    "    dilation_2d = pool2d.dilation if hasattr(pool2d, 'dilation') else 1 # MaxPool2d has dilation\n",
    "    return_indices = pool2d.return_indices if hasattr(pool2d, 'return_indices') else False # MaxPool2d has this\n",
    "    ceil_mode = pool2d.ceil_mode if hasattr(pool2d, 'ceil_mode') else False # Pool2d has this\n",
    "\n",
    "    # Ensure kernel_size, stride, padding are tuples for consistency\n",
    "    if not isinstance(kernel_size_2d, tuple): kernel_size_2d = (kernel_size_2d,) * 2\n",
    "    if not isinstance(stride_2d, tuple): stride_2d = (stride_2d,) * 2\n",
    "    if not isinstance(padding_2d, tuple): padding_2d = (padding_2d,) * 2\n",
    "    if not isinstance(dilation_2d, tuple): dilation_2d = (dilation_2d,) * 2\n",
    "\n",
    "\n",
    "    # Create 3D pooling parameters\n",
    "    # Temporal kernel size (1 for no pooling in time by the pool layer itself, temporal_stride handles downsampling)\n",
    "    kernel_size_3d = (1, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    # Temporal stride for downsampling\n",
    "    stride_3d = (temporal_stride, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding (0 as we don't pool over time)\n",
    "    padding_3d = (0, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "    if isinstance(pool2d, nn.MaxPool2d):\n",
    "        return nn.MaxPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d,\n",
    "                            dilation=dilation_3d, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "    elif isinstance(pool2d, nn.AvgPool2d):\n",
    "        # AvgPool2d also has count_include_pad attribute\n",
    "        count_include_pad = pool2d.count_include_pad if hasattr(pool2d, 'count_include_pad') else True\n",
    "        return nn.AvgPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d, ceil_mode=ceil_mode,\n",
    "                            count_include_pad=count_include_pad) # Count include pad might need adjustment for 3D?\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {type(pool2d)}\")\n",
    "\n",
    "# --- Inflated DenseNet Components ---\n",
    "\n",
    "class InflatedDenseLayer(nn.Module):\n",
    "    def __init__(self, dense_layer2d, conv_kernel_depth=3):\n",
    "        super(InflatedDenseLayer, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in dense_layer2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d):\n",
    "                # For the bottleneck 1x1 conv, kernel_depth is 1\n",
    "                # For the 3x3 conv, kernel_depth is user-specified (default 3)\n",
    "                self.layers.add_module(name, inflate_conv(child, kernel_depth=child.kernel_size[0] if child.kernel_size != (1,1) else 1))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in DenseLayer: {type(child)}\")\n",
    "        # DenseLayer also has a drop_rate attribute\n",
    "        self.drop_rate = dense_layer2d.drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.layers(x)\n",
    "        if self.drop_rate > 0 and self.training: # Apply dropout only during training\n",
    "            new_features = nn.functional.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        # DenseNet connectivity: concatenate input with new features\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class InflatedTransition(nn.Module):\n",
    "    def __init__(self, transition2d, temporal_pool_stride=2):\n",
    "        super(InflatedTransition, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in transition2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d): # This is the 1x1 convolution\n",
    "                 self.layers.add_module(name, inflate_conv(child, kernel_depth=1))\n",
    "            elif isinstance(child, nn.AvgPool2d): # This is the pooling layer for downsampling\n",
    "                self.layers.add_module(name, inflate_pool(child, temporal_stride=temporal_pool_stride))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in Transition: {type(child)}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Main Inflation Function ---\n",
    "\n",
    "def inflate_densenet121(densenet2d, frame_nb, conv_kernel_depth=3, temporal_pool_stride=2,input_channels=1):\n",
    "    \"\"\"\n",
    "    Inflates a torchvision DenseNet121 model to a 3D model.\n",
    "\n",
    "    Args:\n",
    "        densenet2d (torchvision.models.densenet.DenseNet): The pre-trained 2D DenseNet121 model.\n",
    "        frame_nb (int): The expected number of frames in the input video.\n",
    "        conv_kernel_depth (int): The temporal kernel size to use for inflating 2D convs > 1x1.\n",
    "                                 Default is 3 (3x3x3).\n",
    "        temporal_pool_stride (int): The temporal stride to use for inflating spatial pooling\n",
    "                                    layers in transition blocks. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The inflated 3D DenseNet model.\n",
    "    \"\"\"\n",
    "    # Inflate the features part (contains initial conv, pool, dense blocks, transitions)\n",
    "    features_3d = nn.Sequential()\n",
    "    transition_nb = 0\n",
    "    # Flag to identify the very first convolutional layer\n",
    "    is_first_conv_layer = True\n",
    "    for name, child in densenet2d.features.named_children():\n",
    "        if isinstance(child, nn.Conv2d): # Initial Conv2d\n",
    "            if is_first_conv_layer:\n",
    "                 # --- SPECIAL HANDLING FOR THE VERY FIRST CONV LAYER ---\n",
    "                 # The first Conv2d in DenseNet121 has in_channels=3.\n",
    "                 # We need to change its in_channels to 'input_channels' (e.g., 1)\n",
    "                 # and adapt its weights.\n",
    "\n",
    "                 original_conv2d = child\n",
    "                 original_in_channels = original_conv2d.in_channels # This is 3\n",
    "                 if original_in_channels != 3:\n",
    "                     # This inflation logic assumes the original model starts with 3 channels\n",
    "                     warnings.warn(f\"Expected first Conv2d to have 3 input channels, but got {original_in_channels}. Inflation logic might be incorrect.\")\n",
    "\n",
    "                 # Get original parameters except in_channels\n",
    "                 out_channels = original_conv2d.out_channels\n",
    "                 kernel_size_2d = original_conv2d.kernel_size\n",
    "                 stride_2d = original_conv2d.stride\n",
    "                 padding_2d = original_conv2d.padding\n",
    "                 dilation_2d = original_conv2d.dilation\n",
    "                 groups = original_conv2d.groups\n",
    "                 bias = original_conv2d.bias is not None\n",
    "\n",
    "                 # Create the new Conv3d with the DESIRED input_channels\n",
    "                 # The temporal kernel size for the first layer is typically the spatial kernel size (7x7 -> 7x7x7)\n",
    "                 kernel_depth_3d = kernel_size_2d[0] # Use spatial kernel size for time\n",
    "\n",
    "                 kernel_size_3d = (kernel_depth_3d, kernel_size_2d[0], kernel_size_2d[1])\n",
    "                 stride_3d = (1, stride_2d[0], stride_2d[1]) # Temporal stride 1\n",
    "                 padding_3d = (kernel_depth_3d // 2, padding_2d[0], padding_2d[1])\n",
    "                 dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "                 first_conv3d = nn.Conv3d(input_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                                          dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "                 # Inflate weights: Original weights shape [out_channels, 3, H, W]\n",
    "                 original_weights = original_conv2d.weight.data\n",
    "\n",
    "                 if input_channels == 1 and original_in_channels == 3:\n",
    "                     # Average the original 3 input channel weights\n",
    "                     adapted_weights_2d = original_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "                 elif input_channels == original_in_channels:\n",
    "                     # Input channels match, just use original weights\n",
    "                     adapted_weights_2d = original_weights\n",
    "                 else:\n",
    "                     # Handle other input channel numbers if needed (e.g., randomly initialize new weights)\n",
    "                     warnings.warn(f\"Handling inflation from {original_in_channels} to {input_channels} channels. \"\n",
    "                                   \"Using averaged weights if original=3, otherwise weights might need custom handling.\")\n",
    "                     if original_in_channels == 3:\n",
    "                         adapted_weights_2d = original_weights.mean(dim=1, keepdim=True).repeat(1, input_channels, 1, 1) # Repeat averaged weight\n",
    "                     else:\n",
    "                         # Fallback: Random initialization for the new layer if sizes don't match\n",
    "                         print(f\"Initializing weights for first Conv3d ({original_in_channels} -> {input_channels}) randomly.\")\n",
    "                         # The first_conv3d layer is already initialized randomly by default, so nothing more to do here.\n",
    "                         # We can just skip the weight copying step.\n",
    "                         adapted_weights_2d = None # Indicate no specific weights to copy\n",
    "\n",
    "\n",
    "                 if adapted_weights_2d is not None:\n",
    "                    # Repeat the adapted 2D weights along the temporal dimension\n",
    "                    # Shape [out_channels, input_channels, 1, H, W] -> [out_channels, input_channels, D, H, W]\n",
    "                    inflated_weights = adapted_weights_2d.unsqueeze(2).repeat(1, 1, kernel_depth_3d, 1, 1)\n",
    "\n",
    "                    # Normalize by dividing by the depth\n",
    "                    if kernel_depth_3d > 0:\n",
    "                         inflated_weights = inflated_weights / kernel_depth_3d\n",
    "\n",
    "                    # Copy inflated weights to the new Conv3d\n",
    "                    first_conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "                 if bias:\n",
    "                     first_conv3d.bias.data.copy_(original_conv2d.bias.data)\n",
    "\n",
    "                 features_3d.add_module(name, first_conv3d)\n",
    "                 is_first_conv_layer = False # Mark that the first conv is processed\n",
    "\n",
    "            else:\n",
    "                 # --- STANDARD INFLATION FOR SUBSEQUENT CONV LAYERS ---\n",
    "                 # These layers should use the standard inflate_conv logic\n",
    "                 # Their in_channels will match the out_channels of the preceding 3D layer.\n",
    "                 # We still need to handle the kernel depth for non-1x1 convs.\n",
    "                 temporal_k_depth = child.kernel_size[0] if child.kernel_size != (1,1) else 1\n",
    "                 features_3d.add_module(name, inflate_conv_standard(child, kernel_depth=temporal_k_depth))\n",
    "\n",
    "             # The initial convolution in DenseNet121 is 7x7. Inflate it.\n",
    "            \n",
    "        elif isinstance(child, nn.BatchNorm2d): # Initial BatchNorm\n",
    "             features_3d.add_module(name, inflate_batch_norm(child))\n",
    "        elif isinstance(child, nn.ReLU): # Initial ReLU\n",
    "             features_3d.add_module(name, inflate_relu(child))\n",
    "        elif isinstance(child, nn.MaxPool2d): # Initial MaxPooling\n",
    "             # Initial pool typically reduces spatial dimensions but not temporal\n",
    "             # We'll make the temporal stride 1 here\n",
    "             features_3d.add_module(name, inflate_pool(child, temporal_stride=1))\n",
    "        elif isinstance(child, models.densenet._DenseBlock):\n",
    "             # Inflate the DenseBlock\n",
    "             block_3d = nn.Sequential()\n",
    "             for nested_name, nested_child in child.named_children():\n",
    "                 # Each child in a DenseBlock is a DenseLayer\n",
    "                 assert isinstance(nested_child, models.densenet._DenseLayer)\n",
    "                 block_3d.add_module(nested_name, InflatedDenseLayer(nested_child, conv_kernel_depth=conv_kernel_depth))\n",
    "             features_3d.add_module(name, block_3d)\n",
    "        elif isinstance(child, models.densenet._Transition):\n",
    "             # Inflate the Transition layer\n",
    "             features_3d.add_module(name, InflatedTransition(child, temporal_pool_stride=temporal_pool_stride))\n",
    "             transition_nb += 1\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in features: {name} ({type(child)})\")\n",
    "            pass # Skip layers like OrderedDictWrapper if they appear\n",
    "\n",
    "    # Calculate the final temporal dimension\n",
    "    # Assumes each transition block reduces temporal dimension by temporal_pool_stride\n",
    "    temporal_reduction_factor = int(math.pow(temporal_pool_stride, transition_nb))\n",
    "    final_time_dim = frame_nb // temporal_reduction_factor\n",
    "    if frame_nb % temporal_reduction_factor != 0:\n",
    "         warnings.warn(f\"Input frame_nb ({frame_nb}) is not perfectly divisible by temporal reduction factor ({temporal_reduction_factor}). \"\n",
    "                       \"Final temporal dimension will be floor division result.\")\n",
    "\n",
    "\n",
    "    # Inflate the classifier part\n",
    "    classifier_3d = nn.Sequential()\n",
    "    for name, child in densenet2d.classifier.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # The linear layer input size needs to account for the flattened 3D features\n",
    "            # It's final_time_dim * original_classifier_in_features\n",
    "            original_in_features = child.in_features\n",
    "            inflated_in_features = final_time_dim * original_in_features\n",
    "\n",
    "            # Create new 3D linear layer\n",
    "            linear3d = nn.Linear(inflated_in_features, child.out_features, bias=child.bias is not None)\n",
    "\n",
    "            # Inflate weights\n",
    "            linear2d_weights = child.weight.data # Shape [out_features, in_features_2d]\n",
    "\n",
    "            # Reshape 2D weights to [out_features, in_features_2d, 1] (add a temporal dimension)\n",
    "            inflated_weights = linear2d_weights.unsqueeze(2)\n",
    "            # Repeat along the new temporal dimension\n",
    "            inflated_weights = inflated_weights.repeat(1, 1, final_time_dim)\n",
    "            # Reshape to match the 3D linear layer's expected shape [out_features, inflated_in_features]\n",
    "            inflated_weights = inflated_weights.view(child.out_features, inflated_in_features)\n",
    "\n",
    "            linear3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "            # Copy bias if it exists\n",
    "            if child.bias is not None:\n",
    "                linear3d.bias.data.copy_(child.bias.data)\n",
    "\n",
    "            classifier_3d.add_module(name, linear3d)\n",
    "\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in classifier: {name} ({type(child)})\")\n",
    "            pass # DenseNet's classifier is typically just a Linear layer\n",
    "\n",
    "    # Combine features and classifier into a new Sequential model\n",
    "    # We need to manually handle the final pooling and flatten step in the forward pass\n",
    "    # like in the previous I3D code, as Sequential doesn't do this automatically.\n",
    "    # So we'll create a custom module for the final model.\n",
    "\n",
    "    class InflatedDenseNetModel(nn.Module):\n",
    "        def __init__(self, features_3d, classifier_3d, final_time_dim, final_layer_nb):\n",
    "            super().__init__()\n",
    "            self.features = features_3d\n",
    "            self.classifier = classifier_3d\n",
    "            self.final_time_dim = final_time_dim\n",
    "            self.final_layer_nb = final_layer_nb # This is the number of channels before global pooling\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            # Apply ReLU after features (matches typical DenseNet flow before classification)\n",
    "            x = nn.functional.relu(x)\n",
    "            # Global spatial average pooling. Kernel size matches expected output spatial dims.\n",
    "            # Assuming 7x7 spatial dims before classification for DenseNet121 on ImageNet size inputs\n",
    "            spatial_kernel_h = x.shape[-2]\n",
    "            spatial_kernel_w = x.shape[-1]\n",
    "            x = nn.functional.avg_pool3d(x, kernel_size=(1, spatial_kernel_h, spatial_kernel_w))\n",
    "            # Flatten for classifier\n",
    "            # Original shape: [batch, channels, depth, 1, 1] after spatial pooling\n",
    "            # Permute to [batch, depth, channels, 1, 1]\n",
    "            x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "            # View to [batch, depth * channels]\n",
    "            x = x.view(-1, self.final_time_dim * self.final_layer_nb)\n",
    "            # Pass through classifier\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    # Get the number of channels before the original 2D classifier\n",
    "    final_layer_nb = densenet2d.classifier.in_features\n",
    "\n",
    "    return InflatedDenseNetModel(features_3d, classifier_3d, final_time_dim, final_layer_nb)\n",
    "\n",
    "\n",
    "# --- Testing the Inflation ---\n",
    "\n",
    "import warnings\n",
    "\n",
    "# a. Take a 2D pretrained DenseNet121 model\n",
    "model_2d = models.densenet121(pretrained=True)\n",
    "print(\"Loaded 2D DenseNet121 model.\")\n",
    "\n",
    "# Expected number of frames in your video input\n",
    "input_frame_nb = 16\n",
    "# Temporal kernel depth for inflating 3x3 convs\n",
    "conv_k_depth = 3\n",
    "# Temporal stride for pooling in transition blocks\n",
    "pool_t_stride = 2\n",
    "\n",
    "# Inflate the model\n",
    "i3d_densenet_model = inflate_densenet121(\n",
    "    model_2d,\n",
    "    frame_nb=input_frame_nb,\n",
    "    conv_kernel_depth=conv_k_depth,\n",
    "    temporal_pool_stride=pool_t_stride\n",
    ")\n",
    "\n",
    "print(f\"\\nInflated 3D DenseNet model created for {input_frame_nb} frames.\")\n",
    "# Print the structure of the inflated model (optional, can be long)\n",
    "# print(i3d_densenet_model)\n",
    "\n",
    "\n",
    "# Create a dummy 3D input tensor\n",
    "# Shape: [batch_size, channels, depth, height, width]\n",
    "batch_size = 2\n",
    "channels = 1\n",
    "input_height = 224\n",
    "input_width = 224\n",
    "\n",
    "dummy_input_3d = torch.randn(batch_size, channels, input_frame_nb, input_height, input_width)\n",
    "print(\"\\nDummy input shape:\", dummy_input_3d.shape)\n",
    "\n",
    "\n",
    "# Perform a forward pass with the dummy input\n",
    "try:\n",
    "    i3d_densenet_model.eval() # Set model to evaluation mode (disables dropout, uses running stats for BN)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        output = i3d_densenet_model(dummy_input_3d)\n",
    "\n",
    "    print(\"\\nForward pass successful!\")\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # The output shape should be [batch_size, num_classes]\n",
    "    # For DenseNet121 pre-trained on ImageNet, num_classes is 1000\n",
    "    print(\"Expected output shape:\", (batch_size, 1000))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nForward pass failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a9c2f9",
   "metadata": {
    "cellUniqueIdByVincent": "05bbc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'densenet2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 473\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# --- Corrected Classifier Inflation in inflate_densenet121 ---\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# (This part was already corrected in the previous response, but including it for completeness)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# This part goes *after* the features inflation loop in inflate_densenet121\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Get the original 2D Linear classifier module directly\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m original_linear_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mdensenet2d\u001b[49m\u001b[38;5;241m.\u001b[39mclassifier\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(original_linear_classifier, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m    476\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected original classifier to be nn.Linear, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(original_linear_classifier)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'densenet2d' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33e8d3ca",
   "metadata": {
    "cellUniqueIdByVincent": "a7442"
   },
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5771af49",
   "metadata": {
    "cellUniqueIdByVincent": "a334f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3ae840",
   "metadata": {
    "cellUniqueIdByVincent": "8c19f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22ac0c",
   "metadata": {
    "cellUniqueIdByVincent": "b0a54"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7a019",
   "metadata": {
    "cellUniqueIdByVincent": "11363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and inflated successfully.\n",
      "\n",
      "Processing region: Tibia\n",
      "  Processing volume: tibia.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:115: UserWarning: NIfTI volume depth (512) does not match frame_nb (64) used during model inflation. This might cause issues or requires padding/cropping.\n",
      "  warnings.warn(f\"NIfTI volume depth ({current_volume_depth}) does not match frame_nb ({frame_nb}) \"\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:153: UserWarning: Invalid target layer format: third_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:153: UserWarning: Invalid target layer format: fifth_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded volume shape: (512, 512, 216), Prepared tensor shape: torch.Size([1, 1, 512, 512, 216])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import pickle # For saving the dictionary\n",
    "# Assuming your inflate_densenet121 function and helpers are defined here or imported\n",
    "# from your previous work.\n",
    "# from i3d_densenet_inflation import inflate_densenet121\n",
    "import torchvision.models as models # Needed to load the 2D model\n",
    "\n",
    "\n",
    "# --- (Include the inflate_conv, inflate_batch_norm, inflate_relu, inflate_pool,\n",
    "#      InflatedDenseLayer, InflatedTransition, InflatedDenseNetModel, inflate_densenet121\n",
    "#      definitions from previous responses here) ---\n",
    "# Make sure you have the CORRECTED inflate_densenet121 that handles input_channels=1\n",
    "# and the inflate_conv_standard helper.\n",
    "\n",
    "\n",
    "# --- Helper function to find specific convolution layers (same as before) ---\n",
    "\n",
    "def find_convolution_layers(module):\n",
    "    \"\"\"Recursively finds all Conv3d layers within a PyTorch module.\"\"\"\n",
    "    conv_layers = []\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv3d):\n",
    "            conv_layers.append((name, child))\n",
    "        else:\n",
    "            conv_layers.extend(find_convolution_layers(child))\n",
    "    return conv_layers\n",
    "\n",
    "# --- Feature Extraction Function for a Specific Masked Region (same as before) ---\n",
    "# This function remains the same as in the previous response, as its job is to\n",
    "# process one region given the volume data and mask data.\n",
    "\n",
    "def extract_features_from_masked_region(model_3d, volume_data, mask_data, region_name, frame_nb, target_layers):\n",
    "    \"\"\"\n",
    "    Extracts a region from volume_data using mask_data, preprocesses it,\n",
    "    runs it through the 3D CNN features, and extracts GAP features.\n",
    "\n",
    "    Args:\n",
    "        model_3d (torch.nn.Module): The inflated 3D DenseNet model.\n",
    "        volume_data (np.ndarray): NumPy array of the loaded NIfTI volume data [D, H, W] or [D, H, W, C].\n",
    "        mask_data (np.ndarray): NumPy array of the loaded NIfTI mask data for this region [D, H, W].\n",
    "        region_name (str): The name of the region (e.g., 'Tibia').\n",
    "        frame_nb (int): The expected number of frames/slices (CNN's depth) after preprocessing.\n",
    "                         This should match the `frame_nb` used during inflation.\n",
    "        target_layers (list): List of strings specifying which convolution layers\n",
    "                              to extract features from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the target layer names/indices\n",
    "              and values are the corresponding N-dimensional feature vectors\n",
    "              after Global Average Pooling. Returns an empty dictionary if the\n",
    "              region is not found in the mask or processing fails.\n",
    "    Raises:\n",
    "        ValueError: If input shapes are inconsistent or preprocessing fails.\n",
    "        RuntimeError: If a forward pass fails.\n",
    "    \"\"\"\n",
    "    print(f\"      Processing region: {region_name}\")\n",
    "\n",
    "    # Ensure volume and mask shapes are compatible (ignoring potential channel dim in volume)\n",
    "    volume_spatial_shape = volume_data.shape[:3] # Get D, H, W ignoring Channel\n",
    "    mask_spatial_shape = mask_data.shape\n",
    "    if volume_spatial_shape != mask_spatial_shape:\n",
    "        raise ValueError(f\"Volume and mask spatial shapes for region {region_name} do not match: {volume_spatial_shape} vs {mask_spatial_shape}\")\n",
    "\n",
    "    # Find the bounding box of the masked region\n",
    "    coords = np.argwhere(mask_data > 0) # Assuming mask has non-zero values within the region\n",
    "    if coords.shape[0] == 0:\n",
    "        print(f\"        Region {region_name} not found (mask is all zeros). Skipping.\")\n",
    "        return {} # Return empty features for this region\n",
    "\n",
    "    min_coords = coords.min(axis=0)\n",
    "    max_coords = coords.max(axis=0)\n",
    "\n",
    "    # Add a buffer to the bounding box (optional, but often useful)\n",
    "    buffer = 5 # Example buffer\n",
    "    min_coords = np.maximum([0, 0, 0], min_coords - buffer)\n",
    "    max_coords = np.minimum(volume_spatial_shape, max_coords + buffer + 1) # +1 to include max_coords\n",
    "\n",
    "    # Crop the original volume data using the bounding box\n",
    "    if volume_data.ndim == 4: # If original volume had shape [D, H, W, C]\n",
    "         cropped_data = volume_data[min_coords[0]:max_coords[0],\n",
    "                                    min_coords[1]:max_coords[1],\n",
    "                                    min_coords[2]:max_coords[2], :]\n",
    "         num_channels = cropped_data.shape[-1]\n",
    "         print(f\"        Cropped region has {num_channels} channels.\")\n",
    "    else: # Assuming grayscale [D, H, W]\n",
    "         cropped_data = volume_data[min_coords[0]:max_coords[0],\n",
    "                                    min_coords[1]:max_coords[1],\n",
    "                                    min_coords[2]:max_coords[2]]\n",
    "         num_channels = 1\n",
    "\n",
    "\n",
    "    # --- Preprocessing the Cropped Region Data ---\n",
    "    # This is a critical step: the cropped region must match the CNN's expected input dimensions\n",
    "    current_region_depth = cropped_data.shape[0] # Assuming depth is the first dim\n",
    "    target_depth = frame_nb\n",
    "\n",
    "    if current_region_depth != target_depth:\n",
    "         warnings.warn(f\"        Cropped region depth ({current_region_depth}) does not match target frame_nb ({target_depth}). Applying padding/cropping.\")\n",
    "         if current_region_depth < target_depth:\n",
    "             pad_amount = target_depth - current_region_depth\n",
    "             pad_tuple = ((0, pad_amount), (0, 0), (0, 0))\n",
    "             if cropped_data.ndim == 4:\n",
    "                  pad_tuple = ((0, pad_amount), (0, 0), (0, 0), (0, 0))\n",
    "             cropped_data = np.pad(cropped_data, pad_tuple, mode='constant', constant_values=0)\n",
    "         elif current_region_depth > target_depth:\n",
    "             start = (current_region_depth - target_depth) // 2\n",
    "             cropped_data = cropped_data[start:start + target_depth, :, :]\n",
    "\n",
    "    # Implement spatial resizing here if needed.\n",
    "    # target_spatial_size = (224, 224) # Example target spatial size (H, W)\n",
    "    # if cropped_data.shape[1:3] != target_spatial_size:\n",
    "    #      warnings.warn(f\"        Spatial dimensions of cropped region {cropped_data.shape[1:3]} do not match target spatial size {target_spatial_size}. Resizing needed.\")\n",
    "    #      # Example resizing (requires skimage)\n",
    "    #      # from skimage.transform import resize\n",
    "    #      # resize_shape = (cropped_data.shape[0], target_spatial_size[0], target_spatial_size[1])\n",
    "    #      # if cropped_data.ndim == 4: resize_shape += (cropped_data.shape[3],) # Add channel if present\n",
    "    #      # # Resize each channel/slice individually if needed, or use a 4D/5D resize function\n",
    "    #      # # Be very careful with interpolation for medical data! Order 1 or 0 (nearest neighbor) often preferred.\n",
    "    #      # resized_data = resize(cropped_data, resize_shape, order=1, preserve_range=True, anti_aliasing=False)\n",
    "    #      # cropped_data = resized_data.astype(np.float32)\n",
    "    #      pass\n",
    "\n",
    "\n",
    "    # Convert preprocessed data to PyTorch tensor\n",
    "    if cropped_data.ndim == 3: # Grayscale [D, H, W] after preprocessing\n",
    "         input_tensor = torch.from_numpy(cropped_data).unsqueeze(0).unsqueeze(0) # -> [1, 1, D, H, W]\n",
    "    elif cropped_data.ndim == 4: # [D, H, W, C] after preprocessing\n",
    "         input_tensor = torch.from_numpy(cropped_data).permute(3, 0, 1, 2).unsqueeze(0) # -> [1, C, D, H, W]\n",
    "    else:\n",
    "         raise ValueError(f\"Unexpected shape after preprocessing: {cropped_data.shape}\")\n",
    "\n",
    "    print(f\"        Preprocessed tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "    # --- Feature Extraction ---\n",
    "\n",
    "    # Find all Conv3d layers and their names (assuming model_3d.features holds the CNN backbone)\n",
    "    all_conv_layers = find_convolution_layers(model_3d.features)\n",
    "    if not all_conv_layers:\n",
    "        print(f\"        No Conv3d layers found in the model's features. Skipping {region_name}.\")\n",
    "        return {}\n",
    "\n",
    "    # Map target layer names/indices to actual layers\n",
    "    layers_to_extract = {}\n",
    "    num_conv_layers = len(all_conv_layers)\n",
    "    target_layers = ['last', 'third_last', 'fifth_last']\n",
    "    for layer_name, layer_module in all_conv_layers:\n",
    "        if layer_name in target_layers:\n",
    "            layers_to_extract[layer_name] = (layer_name, layer_module)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        warnings.warn(f\"Target layer '{target}' not found by name.\")\n",
    "\n",
    "    if not layers_to_extract:\n",
    "        print(f\"        No valid target layers specified or found for region {region_name}. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "    # Hook to capture intermediate feature maps\n",
    "    feature_maps = {}\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "         layer_info = None\n",
    "         for name, layer_module in layers_to_extract.values():\n",
    "              if layer_module is module:\n",
    "                  layer_info = (name, type(layer_module).__name__)\n",
    "                  break\n",
    "         if layer_info:\n",
    "              feature_maps[layer_info] = output.clone() # Capture output\n",
    "\n",
    "\n",
    "    for key, (layer_name, layer_module) in layers_to_extract.items():\n",
    "         hooks.append(layer_module.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Set model to evaluation mode and disable gradients\n",
    "    model_3d.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Pass the preprocessed region tensor through the feature extraction layers\n",
    "            _ = model_3d.features(input_tensor)\n",
    "        except Exception as e:\n",
    "            for h in hooks: h.remove()\n",
    "            raise RuntimeError(f\"      Forward pass failed for region {region_name}: {e}\")\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # Apply Global Average Pooling (GAP) to extracted feature maps\n",
    "    extracted_features = {}\n",
    "    for layer_info, feature_map in feature_maps.items():\n",
    "        if feature_map.dim() != 5:\n",
    "             warnings.warn(f\"        Feature map for layer {layer_info} has unexpected dimensions {feature_map.shape}. Skipping GAP.\")\n",
    "             continue\n",
    "\n",
    "        pool_kernel_size = feature_map.size()[2:]\n",
    "        if 0 in pool_kernel_size:\n",
    "             warnings.warn(f\"        Pooling kernel size contains 0 for layer {layer_info} ({pool_kernel_size}). Skipping GAP.\")\n",
    "             continue\n",
    "\n",
    "        pooled_features = nn.functional.avg_pool3d(feature_map, kernel_size=pool_kernel_size)\n",
    "        feature_vector = pooled_features.squeeze(-1).squeeze(-1).squeeze(-1) # Shape [batch_size, channels]\n",
    "\n",
    "        if feature_vector.shape[0] == 1:\n",
    "             feature_vector = feature_vector.squeeze(0) # Shape [channels]\n",
    "\n",
    "        extracted_features[layer_info] = feature_vector.cpu().numpy() # Convert to NumPy and move to CPU for saving\n",
    "\n",
    "\n",
    "    return extracted_features\n",
    "\n",
    "# --- Main Script to Process One Volume and Save Features ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Directory containing your single volume and its mask files\n",
    "    data_dir = './my_medical_scan/' # Example: Contains my_scan.nii.gz, my_scan_tibia.nii.gz, etc.\n",
    "\n",
    "    # Base name of your volume file (without extension or region suffix)\n",
    "    volume_base_name = 'my_scan' # Example: Your volume is named 'my_scan.nii.gz'\n",
    "\n",
    "    # List of regions you have separate masks for (corresponding to suffixes like '_tibia')\n",
    "    regions_to_process = ['tibia', 'femur', 'background']\n",
    "\n",
    "    # Expected number of frames/slices (CNN's depth) *after* padding/cropping of regions\n",
    "    # This MUST match the frame_nb used when inflating the model\n",
    "    input_frame_nb_for_cnn = 64 # Example\n",
    "\n",
    "    # Define the layers you want to extract features from\n",
    "    layers_to_get = ['last', 'third_last', 'fifth_last'] # Example targets\n",
    "\n",
    "    # Output directory to save the features\n",
    "    output_dir = './extracted_features/'\n",
    "    os.makedirs(output_dir, exist_ok=True) # Create output directory if it doesn't exist\n",
    "\n",
    "    # --- Load and Inflate the Model ---\n",
    "    try:\n",
    "        model_2d = models.densenet121(pretrained=True)\n",
    "        # Inflate with the target depth and 1 input channel for grayscale\n",
    "        i3d_densenet_model = inflate_densenet121(\n",
    "            model_2d,\n",
    "            frame_nb=input_frame_nb_for_cnn,\n",
    "            input_channels=1 # Specify 1 input channel for grayscale\n",
    "        )\n",
    "        print(\"Model loaded and inflated successfully with 1 input channel.\")\n",
    "        # Optional: Move model to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        i3d_densenet_model.to(device)\n",
    "        print(f\"Model moved to device: {device}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or inflating model: {e}\")\n",
    "        exit() # Exit if model setup fails\n",
    "\n",
    "    # --- Process the single volume ---\n",
    "    volume_path = os.path.join(data_dir, f'{volume_base_name}.nii.gz')\n",
    "    if not os.path.exists(volume_path):\n",
    "        volume_path = os.path.join(data_dir, f'{volume_base_name}.nii') # Try .nii\n",
    "        if not os.path.exists(volume_path):\n",
    "             print(f\"\\nOriginal volume file not found for base name {volume_base_name}. Exiting.\")\n",
    "             exit()\n",
    "\n",
    "    print(f\"\\nProcessing volume: {volume_base_name}\")\n",
    "    all_region_features_for_volume = {} # Store features for this volume across all regions\n",
    "\n",
    "    try:\n",
    "        # Load the original volume data once\n",
    "        volume_img = nib.load(volume_path)\n",
    "        volume_data = volume_img.get_fdata().astype(np.float32)\n",
    "        print(f\"  Loaded original volume data with shape: {volume_data.shape}\")\n",
    "        # Optional: Move volume data to GPU before preprocessing if needed\n",
    "        # volume_data_tensor = torch.from_numpy(volume_data).to(device)\n",
    "\n",
    "        # Process each region for this volume\n",
    "        for region in regions_to_process:\n",
    "            mask_path = os.path.join(data_dir, f'{volume_base_name}_{region}.nii.gz')\n",
    "            if not os.path.exists(mask_path):\n",
    "                 mask_path = os.path.join(data_dir, f'{volume_base_name}_{region}.nii') # Try .nii\n",
    "                 if not os.path.exists(mask_path):\n",
    "                      print(f\"    Mask file not found for region {region} at {mask_path}. Skipping region.\")\n",
    "                      all_region_features_for_volume[region] = {} # Store empty features for this region\n",
    "                      continue\n",
    "\n",
    "            # Load the mask data\n",
    "            mask_img = nib.load(mask_path)\n",
    "            mask_data = mask_img.get_fdata() # Mask data can be int or bool\n",
    "\n",
    "            # Extract features from the masked region\n",
    "            extracted_feats = extract_features_from_masked_region(\n",
    "                i3d_densenet_model,\n",
    "                volume_data, # Use the original NumPy data\n",
    "                mask_data,\n",
    "                region_name=region,\n",
    "                frame_nb=input_frame_nb_for_cnn,\n",
    "                target_layers=layers_to_get\n",
    "            )\n",
    "            all_region_features_for_volume[region] = extracted_feats\n",
    "\n",
    "        print(f\"  Finished processing regions for volume {volume_base_name}\")\n",
    "\n",
    "        # --- Save the extracted features ---\n",
    "        output_filepath = os.path.join(output_dir, f'{volume_base_name}_features.pkl')\n",
    "        with open(output_filepath, 'wb') as f:\n",
    "            pickle.dump(all_region_features_for_volume, f)\n",
    "        print(f\"  Saved extracted features for volume {volume_base_name} to {output_filepath}\")\n",
    "\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError, TypeError) as e:\n",
    "        print(f\"  Error processing volume {volume_base_name}: {e}\")\n",
    "        # Continue wouldn't apply here as we only have one volume, but good for error handling\n",
    "\n",
    "    print(\"\\n--- Region Feature Extraction Complete ---\")\n",
    "\n",
    "    # You can now load the features later using pickle:\n",
    "    # with open('./extracted_features/my_scan_features.pkl', 'rb') as f:\n",
    "    #     loaded_features = pickle.load(f)\n",
    "    #\n",
    "    # # Access features, e.g., for Tibia, last Conv3d:\n",
    "    # tibia_feats = loaded_features['tibia']\n",
    "    # for key, feat in tibia_feats.items():\n",
    "    #     if key[0] == 'last':\n",
    "    #          print(\"\\nLoaded Tibia features (last Conv3d):\", feat)\n",
    "    #          print(\"Shape:\", feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9acc4e1",
   "metadata": {
    "cellUniqueIdByVincent": "77828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and inflated successfully.\n",
      "  Loaded original volume data with shape: (512, 512, 216)\n",
      "      Processing region: tibia\n",
      "        Preprocessed tensor shape: torch.Size([1, 1, 16, 101, 216])\n",
      "  Error processing volume:       Forward pass failed for region tibia: Given groups=1, weight of size [64, 3, 7, 7, 7], expected input[1, 1, 16, 101, 216] to have 3 channels, but got 1 channels instead\n",
      "\n",
      "--- Region Feature Extraction Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:96: UserWarning:         Cropped region depth (116) does not match target frame_nb (16). Applying padding/cropping.\n",
      "  warnings.warn(f\"        Cropped region depth ({current_region_depth}) does not match target frame_nb ({target_depth}). Applying padding/cropping.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:159: UserWarning: Invalid target layer format: third_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:159: UserWarning: Invalid target layer format: fifth_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f700c",
   "metadata": {
    "cellUniqueIdByVincent": "664cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b71715af",
   "metadata": {
    "cellUniqueIdByVincent": "0465b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Background_sample_volume.nii.gz',\n",
       "  {('conv2',\n",
       "    'Conv3d'): tensor([ 0.0051,  0.0122, -0.0055, -0.0065,  0.0012, -0.0178, -0.0320, -0.0056,\n",
       "           -0.0361, -0.0274, -0.0244, -0.0153, -0.0003, -0.0026, -0.0136,  0.0120,\n",
       "            0.0023,  0.0165, -0.0097, -0.0062, -0.0051,  0.0130, -0.0266,  0.0109,\n",
       "           -0.0180, -0.0190, -0.0017,  0.0228, -0.0170,  0.0546,  0.0062,  0.0015])})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e082bee",
   "metadata": {
    "cellUniqueIdByVincent": "a6d4f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "vincent": {
   "sessionId": "5f975c07cdcecf219e5495ea_2025-06-09T08-08-54-758Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
