{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa07d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_file = \"../Task1/image/left_knee.nii.gz\"\n",
    "mask_file = \"..//Task1/output/bone_segmentation_task1_1.nii.gz\"\n",
    "output_folder = \"segmented_regions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image_data,name):\n",
    "    # --- Choose slices to visualize ---\n",
    "    num_slices = 9\n",
    "    slice_indices = np.linspace(0, image_data.shape[2] - 1, num_slices, dtype=int)\n",
    "\n",
    "    # --- Plot slices ---\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        axes[i].imshow(image_data[:, :, slice_idx], cmap='gray')\n",
    "        axes[i].set_title(f\"Slice {slice_idx}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.savefig(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file and return the data array and affine.\n",
    "    \"\"\"\n",
    "    nifti = nib.load(file_path)\n",
    "    data = nifti.get_fdata()\n",
    "    affine = nifti.affine\n",
    "    return data, affine\n",
    "\n",
    "\n",
    "def save_nifti(data, affine, output_path):\n",
    "    \"\"\"\n",
    "    Save a NumPy array as a NIfTI file.\n",
    "    \"\"\"\n",
    "    nifti = nib.Nifti1Image(data.astype(np.float32), affine)\n",
    "    nib.save(nifti, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data,_=load_nifti(ct_file)\n",
    "visualize(image_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_knee_regions(ct_path, mask_path, output_dir):\n",
    "    \"\"\"\n",
    "    Segment Tibia, Femur, and Background from a CT volume using the mask.\n",
    "    \"\"\"\n",
    "    # Load the CT scan and segmentation mask\n",
    "    ct_data, ct_affine = load_nifti(ct_path)\n",
    "    mask_data, _ = load_nifti(mask_path)\n",
    "\n",
    "    # Define region labels (change if your mask uses different values)\n",
    "    TIBIA_LABEL = 1\n",
    "    FEMUR_LABEL = 2\n",
    "    BACKGROUND_LABEL = 0\n",
    "\n",
    "    # Generate binary masks\n",
    "    tibia_mask = (mask_data == TIBIA_LABEL)\n",
    "    femur_mask = (mask_data == FEMUR_LABEL)\n",
    "    background_mask = (mask_data == BACKGROUND_LABEL)\n",
    "\n",
    "    # Apply masks to CT data\n",
    "    tibia_volume = ct_data * tibia_mask\n",
    "    femur_volume = ct_data * femur_mask\n",
    "    background_volume = ct_data * background_mask\n",
    "\n",
    "    # Save each region as a new .nii.gz file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_nifti(tibia_volume, ct_affine, os.path.join(output_dir, \"tibia_volume.nii.gz\"))\n",
    "    save_nifti(femur_volume, ct_affine, os.path.join(output_dir, \"femur_volume.nii.gz\"))\n",
    "    save_nifti(background_volume, ct_affine, os.path.join(output_dir, \"background_volume.nii.gz\"))\n",
    "\n",
    "    print(\"Segmentation done. Files saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_knee_regions(ct_file, mask_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ead239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image_slice, mask_slice, alpha=0.4, cmap_img='gray', cmap_mask='jet'):\n",
    "    \"\"\"\n",
    "    Overlay a segmentation mask on a single image slice.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_slice: 2D numpy array of the image slice.\n",
    "    - mask_slice: 2D numpy array of the mask slice (same size as image_slice).\n",
    "    - alpha: float, transparency of the mask overlay.\n",
    "    - cmap_img: str, colormap for the image.\n",
    "    - cmap_mask: str, colormap for the mask.\n",
    "    \"\"\"\n",
    "    plt.imshow(image_slice, cmap=cmap_img)\n",
    "    if mask_slice is not None and np.any(mask_slice):\n",
    "        plt.imshow(mask_slice, cmap=cmap_mask, alpha=alpha)\n",
    "    plt.axis('off')\n",
    "\n",
    "def visualize_nifti_with_mask(image_path, mask_path=None, num_slices=9, axis=2):\n",
    "    \"\"\"\n",
    "    Visualize NIfTI image slices with optional mask overlays.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image .nii.gz file.\n",
    "    - mask_path: str or None, path to the mask .nii.gz file (optional).\n",
    "    - num_slices: int, number of slices to visualize.\n",
    "    - axis: int, axis along which to slice (0=sagittal, 1=coronal, 2=axial).\n",
    "    \"\"\"\n",
    "    # Load image and optional mask\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    assert mask is None or image.shape == mask.shape, \"Image and mask must have the same shape\"\n",
    "\n",
    "    # Select slices evenly along the chosen axis\n",
    "    slice_indices = np.linspace(0, image.shape[axis] - 1, num_slices, dtype=int)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(slice_indices):\n",
    "        ax = axes[i]\n",
    "        plt.sca(ax)\n",
    "\n",
    "        if axis == 0:\n",
    "            img_slice = image[idx, :, :]\n",
    "            msk_slice = mask[idx, :, :] if mask is not None else None\n",
    "        elif axis == 1:\n",
    "            img_slice = image[:, idx, :]\n",
    "            msk_slice = mask[:, idx, :] if mask is not None else None\n",
    "        else:  # default to axial\n",
    "            img_slice = image[:, :, idx]\n",
    "            msk_slice = mask[:, :, idx] if mask is not None else None\n",
    "\n",
    "        overlay_mask(img_slice, msk_slice)\n",
    "        ax.set_title(f\"Slice {idx}\")\n",
    "    plt.savefig('slices with masks ')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nifti_with_mask(\n",
    "    ct_file,\n",
    "    mask_file,  # Set to None if you don’t have a mask\n",
    "    num_slices=9,\n",
    "    axis=2  # axial view\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_mask_overlay(image_path, mask_path, slice_index, axis=2, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Plot image slice, mask slice, and overlay side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: 3D numpy array (CT volume)\n",
    "    - mask: 3D numpy array (segmentation mask)\n",
    "    - slice_index: int, index of the slice to visualize\n",
    "    - axis: int, slicing axis (0=sagittal, 1=coronal, 2=axial)\n",
    "    - alpha: float, transparency for overlay\n",
    "    \"\"\"\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    # Extract slices\n",
    "    if axis == 0:\n",
    "        img_slice = image[slice_index, :, :]\n",
    "        mask_slice = mask[slice_index, :, :]\n",
    "    elif axis == 1:\n",
    "        img_slice = image[:, slice_index, :]\n",
    "        mask_slice = mask[:, slice_index, :]\n",
    "    else:\n",
    "        img_slice = image[:, :, slice_index]\n",
    "        mask_slice = mask[:, :, slice_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(img_slice, cmap='gray')\n",
    "    axs[0].set_title(f'Image Slice {slice_index}')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(mask_slice, cmap='jet')\n",
    "    axs[1].set_title(f'Mask Slice {slice_index}')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(img_slice, cmap='gray')\n",
    "    axs[2].imshow(mask_slice, cmap='jet', alpha=alpha)\n",
    "    axs[2].set_title('Overlay')\n",
    "    axs[2].axis('off')\n",
    "    plt.savefig('slice_108_with_maska')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_index = 108  \n",
    "axis = 2  # Axial view\n",
    "\n",
    "plot_image_mask_overlay(ct_file, mask_file, slice_index, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34ea979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/image/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49713466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf5d25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2D DenseNet121 model.\n",
      "\n",
      "Inflated 3D DenseNet model created for 16 frames.\n",
      "\n",
      "Dummy input shape: torch.Size([2, 1, 16, 224, 224])\n",
      "\n",
      "Forward pass successful!\n",
      "Output shape: torch.Size([2, 2048])\n",
      "Expected output shape: (2, 1000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "# --- Helper Functions for Inflation ---\n",
    "\n",
    "def inflate_conv(conv2d, kernel_depth):\n",
    "    \"\"\"Inflates a Conv2d layer to a Conv3d layer by repeating weights.\"\"\"\n",
    "    if conv2d.in_channels % conv2d.groups != 0 or conv2d.out_channels % conv2d.groups != 0:\n",
    "         raise ValueError(\"Inflating grouped convolutions is not straightforward with simple repetition.\")\n",
    "\n",
    "    # Get 2D parameters\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size_2d = conv2d.kernel_size\n",
    "    stride_2d = conv2d.stride\n",
    "    padding_2d = conv2d.padding\n",
    "    dilation_2d = conv2d.dilation\n",
    "    groups = conv2d.groups\n",
    "    bias = conv2d.bias is not None\n",
    "\n",
    "    # Create 3D convolution parameters\n",
    "    # Temporal kernel size\n",
    "    kernel_size_3d = (kernel_depth, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    \n",
    "    # Typically stride 1 in time for simple inflation unless downsampling is desired\n",
    "    # Based on the previous I3D code structure, transitions downsample spatially,\n",
    "    # and the initial conv might downsample spatially. Temporal downsampling is handled\n",
    "    # by the pooling layers in transition blocks.\n",
    "    # So we'll use a temporal stride of 1 here for convolution inflation.\n",
    "    stride_3d = (1, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding to keep temporal dimension size\n",
    "    padding_3d = (kernel_depth // 2, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1]) # Dilation usually 1 in time\n",
    "\n",
    "    # Create 3D Conv layer\n",
    "    conv3d = nn.Conv3d(in_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                       dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "    # Inflate weights (replication and scaling)\n",
    "    conv2d_weights = conv2d.weight.data #  Shape [out_channels, 3, H, W]\n",
    "    #averaged_weights = conv2d_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "\n",
    "                 # Now repeat this 1-channel weight along the temporal dimension\n",
    "    inflated_weights = conv2d_weights.unsqueeze(2).repeat(1, 1, kernel_depth, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize by dividing by the depth (as per requirement ii)\n",
    "    if kernel_depth > 0: # Avoid division by zero if somehow kernel_depth is 0\n",
    "        inflated_weights = inflated_weights / kernel_depth\n",
    "\n",
    "    # Copy inflated weights to the 3D Conv layer\n",
    "    conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "    # Copy bias if it exists\n",
    "    if bias:\n",
    "        conv3d.bias.data.copy_(conv2d.bias.data)\n",
    "\n",
    "    return conv3d\n",
    "\n",
    "def inflate_batch_norm(bn2d):\n",
    "    \"\"\"Inflates a BatchNorm2d layer to a BatchNorm3d layer.\"\"\"\n",
    "    bn3d = nn.BatchNorm3d(bn2d.num_features)\n",
    "    # Copy parameters and running statistics\n",
    "    bn3d.weight.data.copy_(bn2d.weight.data)\n",
    "    bn3d.bias.data.copy_(bn2d.bias.data)\n",
    "    bn3d.running_mean.copy_(bn2d.running_mean)\n",
    "    bn3d.running_var.copy_(bn2d.running_var)\n",
    "    # bn3d.num_batches_tracked.copy_(bn2d.num_batches_tracked) # Copy if your BN uses this\n",
    "    return bn3d\n",
    "\n",
    "def inflate_relu(relu2d):\n",
    "    \"\"\"Returns a ReLU layer (same for 2D and 3D).\"\"\"\n",
    "    return nn.ReLU(inplace=relu2d.inplace)\n",
    "\n",
    "def inflate_pool(pool2d, temporal_stride=1):\n",
    "    \"\"\"Inflates a Pooling layer to a 3D Pooling layer.\"\"\"\n",
    "    # Get 2D parameters\n",
    "    kernel_size_2d = pool2d.kernel_size\n",
    "    stride_2d = pool2d.stride\n",
    "    padding_2d = pool2d.padding\n",
    "    dilation_2d = pool2d.dilation if hasattr(pool2d, 'dilation') else 1 # MaxPool2d has dilation\n",
    "    return_indices = pool2d.return_indices if hasattr(pool2d, 'return_indices') else False # MaxPool2d has this\n",
    "    ceil_mode = pool2d.ceil_mode if hasattr(pool2d, 'ceil_mode') else False # Pool2d has this\n",
    "\n",
    "    # Ensure kernel_size, stride, padding are tuples for consistency\n",
    "    if not isinstance(kernel_size_2d, tuple): kernel_size_2d = (kernel_size_2d,) * 2\n",
    "    if not isinstance(stride_2d, tuple): stride_2d = (stride_2d,) * 2\n",
    "    if not isinstance(padding_2d, tuple): padding_2d = (padding_2d,) * 2\n",
    "    if not isinstance(dilation_2d, tuple): dilation_2d = (dilation_2d,) * 2\n",
    "\n",
    "\n",
    "    # Create 3D pooling parameters\n",
    "    # Temporal kernel size (1 for no pooling in time by the pool layer itself, temporal_stride handles downsampling)\n",
    "    kernel_size_3d = (1, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    # Temporal stride for downsampling\n",
    "    stride_3d = (temporal_stride, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding (0 as we don't pool over time)\n",
    "    padding_3d = (0, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "    if isinstance(pool2d, nn.MaxPool2d):\n",
    "        return nn.MaxPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d,\n",
    "                            dilation=dilation_3d, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "    elif isinstance(pool2d, nn.AvgPool2d):\n",
    "        # AvgPool2d also has count_include_pad attribute\n",
    "        count_include_pad = pool2d.count_include_pad if hasattr(pool2d, 'count_include_pad') else True\n",
    "        return nn.AvgPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d, ceil_mode=ceil_mode,\n",
    "                            count_include_pad=count_include_pad) # Count include pad might need adjustment for 3D?\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {type(pool2d)}\")\n",
    "\n",
    "# --- Inflated DenseNet Components ---\n",
    "\n",
    "class InflatedDenseLayer(nn.Module):\n",
    "    def __init__(self, dense_layer2d, conv_kernel_depth=3):\n",
    "        super(InflatedDenseLayer, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in dense_layer2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d):\n",
    "                # For the bottleneck 1x1 conv, kernel_depth is 1\n",
    "                # For the 3x3 conv, kernel_depth is user-specified (default 3)\n",
    "                self.layers.add_module(name, inflate_conv(child, kernel_depth=child.kernel_size[0] if child.kernel_size != (1,1) else 1))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in DenseLayer: {type(child)}\")\n",
    "        # DenseLayer also has a drop_rate attribute\n",
    "        self.drop_rate = dense_layer2d.drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.layers(x)\n",
    "        if self.drop_rate > 0 and self.training: # Apply dropout only during training\n",
    "            new_features = nn.functional.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        # DenseNet connectivity: concatenate input with new features\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class InflatedTransition(nn.Module):\n",
    "    def __init__(self, transition2d, temporal_pool_stride=2):\n",
    "        super(InflatedTransition, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in transition2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d): # This is the 1x1 convolution\n",
    "                 self.layers.add_module(name, inflate_conv(child, kernel_depth=1))\n",
    "            elif isinstance(child, nn.AvgPool2d): # This is the pooling layer for downsampling\n",
    "                self.layers.add_module(name, inflate_pool(child, temporal_stride=temporal_pool_stride))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in Transition: {type(child)}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Main Inflation Function ---\n",
    "\n",
    "def inflate_densenet121(densenet2d, frame_nb, conv_kernel_depth=3, temporal_pool_stride=2,input_channels=1):\n",
    "    \"\"\"\n",
    "    Inflates a torchvision DenseNet121 model to a 3D model.\n",
    "\n",
    "    Args:\n",
    "        densenet2d (torchvision.models.densenet.DenseNet): The pre-trained 2D DenseNet121 model.\n",
    "        frame_nb (int): The expected number of frames in the input video.\n",
    "        conv_kernel_depth (int): The temporal kernel size to use for inflating 2D convs > 1x1.\n",
    "                                 Default is 3 (3x3x3).\n",
    "        temporal_pool_stride (int): The temporal stride to use for inflating spatial pooling\n",
    "                                    layers in transition blocks. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The inflated 3D DenseNet model.\n",
    "    \"\"\"\n",
    "    # Inflate the features part (contains initial conv, pool, dense blocks, transitions)\n",
    "    features_3d = nn.Sequential()\n",
    "    transition_nb = 0\n",
    "    # Flag to identify the very first convolutional layer\n",
    "    is_first_conv_layer = True\n",
    "    for name, child in densenet2d.features.named_children():\n",
    "        if isinstance(child, nn.Conv2d): # Initial Conv2d\n",
    "            if is_first_conv_layer:\n",
    "                 # --- SPECIAL HANDLING FOR THE VERY FIRST CONV LAYER ---\n",
    "                 # The first Conv2d in DenseNet121 has in_channels=3.\n",
    "                 # We need to change its in_channels to 'input_channels' (e.g., 1)\n",
    "                 # and adapt its weights.\n",
    "\n",
    "                 original_conv2d = child\n",
    "                 original_in_channels = original_conv2d.in_channels # This is 3\n",
    "                 if original_in_channels != 3:\n",
    "                     # This inflation logic assumes the original model starts with 3 channels\n",
    "                     warnings.warn(f\"Expected first Conv2d to have 3 input channels, but got {original_in_channels}. Inflation logic might be incorrect.\")\n",
    "\n",
    "                 # Get original parameters except in_channels\n",
    "                 out_channels = original_conv2d.out_channels\n",
    "                 kernel_size_2d = original_conv2d.kernel_size\n",
    "                 stride_2d = original_conv2d.stride\n",
    "                 padding_2d = original_conv2d.padding\n",
    "                 dilation_2d = original_conv2d.dilation\n",
    "                 groups = original_conv2d.groups\n",
    "                 bias = original_conv2d.bias is not None\n",
    "\n",
    "                 # Create the new Conv3d with the DESIRED input_channels\n",
    "                 # The temporal kernel size for the first layer is typically the spatial kernel size (7x7 -> 7x7x7)\n",
    "                 kernel_depth_3d = kernel_size_2d[0] # Use spatial kernel size for time\n",
    "\n",
    "                 kernel_size_3d = (kernel_depth_3d, kernel_size_2d[0], kernel_size_2d[1])\n",
    "                 stride_3d = (1, stride_2d[0], stride_2d[1]) # Temporal stride 1\n",
    "                 padding_3d = (kernel_depth_3d // 2, padding_2d[0], padding_2d[1])\n",
    "                 dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "                 first_conv3d = nn.Conv3d(input_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                                          dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "                 # Inflate weights: Original weights shape [out_channels, 3, H, W]\n",
    "                 original_weights = original_conv2d.weight.data\n",
    "\n",
    "                 if input_channels == 1 and original_in_channels == 3:\n",
    "                     # Average the original 3 input channel weights\n",
    "                     adapted_weights_2d = original_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "                 elif input_channels == original_in_channels:\n",
    "                     # Input channels match, just use original weights\n",
    "                     adapted_weights_2d = original_weights\n",
    "                 else:\n",
    "                     # Handle other input channel numbers if needed (e.g., randomly initialize new weights)\n",
    "                     warnings.warn(f\"Handling inflation from {original_in_channels} to {input_channels} channels. \"\n",
    "                                   \"Using averaged weights if original=3, otherwise weights might need custom handling.\")\n",
    "                     if original_in_channels == 3:\n",
    "                         adapted_weights_2d = original_weights.mean(dim=1, keepdim=True).repeat(1, input_channels, 1, 1) # Repeat averaged weight\n",
    "                     else:\n",
    "                         # Fallback: Random initialization for the new layer if sizes don't match\n",
    "                         print(f\"Initializing weights for first Conv3d ({original_in_channels} -> {input_channels}) randomly.\")\n",
    "                         # The first_conv3d layer is already initialized randomly by default, so nothing more to do here.\n",
    "                         # We can just skip the weight copying step.\n",
    "                         adapted_weights_2d = None # Indicate no specific weights to copy\n",
    "\n",
    "\n",
    "                 if adapted_weights_2d is not None:\n",
    "                    # Repeat the adapted 2D weights along the temporal dimension\n",
    "                    # Shape [out_channels, input_channels, 1, H, W] -> [out_channels, input_channels, D, H, W]\n",
    "                    inflated_weights = adapted_weights_2d.unsqueeze(2).repeat(1, 1, kernel_depth_3d, 1, 1)\n",
    "\n",
    "                    # Normalize by dividing by the depth\n",
    "                    if kernel_depth_3d > 0:\n",
    "                         inflated_weights = inflated_weights / kernel_depth_3d\n",
    "\n",
    "                    # Copy inflated weights to the new Conv3d\n",
    "                    first_conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "                 if bias:\n",
    "                     first_conv3d.bias.data.copy_(original_conv2d.bias.data)\n",
    "\n",
    "                 features_3d.add_module(name, first_conv3d)\n",
    "                 is_first_conv_layer = False # Mark that the first conv is processed\n",
    "\n",
    "            else:\n",
    "                 # --- STANDARD INFLATION FOR SUBSEQUENT CONV LAYERS ---\n",
    "                 # These layers should use the standard inflate_conv logic\n",
    "                 # Their in_channels will match the out_channels of the preceding 3D layer.\n",
    "                 # We still need to handle the kernel depth for non-1x1 convs.\n",
    "                 temporal_k_depth = child.kernel_size[0] if child.kernel_size != (1,1) else 1\n",
    "                 features_3d.add_module(name, inflate_conv_standard(child, kernel_depth=temporal_k_depth))\n",
    "\n",
    "             # The initial convolution in DenseNet121 is 7x7. Inflate it.\n",
    "            \n",
    "        elif isinstance(child, nn.BatchNorm2d): # Initial BatchNorm\n",
    "             features_3d.add_module(name, inflate_batch_norm(child))\n",
    "        elif isinstance(child, nn.ReLU): # Initial ReLU\n",
    "             features_3d.add_module(name, inflate_relu(child))\n",
    "        elif isinstance(child, nn.MaxPool2d): # Initial MaxPooling\n",
    "             # Initial pool typically reduces spatial dimensions but not temporal\n",
    "             # We'll make the temporal stride 1 here\n",
    "             features_3d.add_module(name, inflate_pool(child, temporal_stride=1))\n",
    "        elif isinstance(child, models.densenet._DenseBlock):\n",
    "             # Inflate the DenseBlock\n",
    "             block_3d = nn.Sequential()\n",
    "             for nested_name, nested_child in child.named_children():\n",
    "                 # Each child in a DenseBlock is a DenseLayer\n",
    "                 assert isinstance(nested_child, models.densenet._DenseLayer)\n",
    "                 block_3d.add_module(nested_name, InflatedDenseLayer(nested_child, conv_kernel_depth=conv_kernel_depth))\n",
    "             features_3d.add_module(name, block_3d)\n",
    "        elif isinstance(child, models.densenet._Transition):\n",
    "             # Inflate the Transition layer\n",
    "             features_3d.add_module(name, InflatedTransition(child, temporal_pool_stride=temporal_pool_stride))\n",
    "             transition_nb += 1\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in features: {name} ({type(child)})\")\n",
    "            pass # Skip layers like OrderedDictWrapper if they appear\n",
    "\n",
    "    # Calculate the final temporal dimension\n",
    "    # Assumes each transition block reduces temporal dimension by temporal_pool_stride\n",
    "    temporal_reduction_factor = int(math.pow(temporal_pool_stride, transition_nb))\n",
    "    final_time_dim = frame_nb // temporal_reduction_factor\n",
    "    if frame_nb % temporal_reduction_factor != 0:\n",
    "         warnings.warn(f\"Input frame_nb ({frame_nb}) is not perfectly divisible by temporal reduction factor ({temporal_reduction_factor}). \"\n",
    "                       \"Final temporal dimension will be floor division result.\")\n",
    "\n",
    "\n",
    "    # Inflate the classifier part\n",
    "    classifier_3d = nn.Sequential()\n",
    "    for name, child in densenet2d.classifier.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # The linear layer input size needs to account for the flattened 3D features\n",
    "            # It's final_time_dim * original_classifier_in_features\n",
    "            original_in_features = child.in_features\n",
    "            inflated_in_features = final_time_dim * original_in_features\n",
    "\n",
    "            # Create new 3D linear layer\n",
    "            linear3d = nn.Linear(inflated_in_features, child.out_features, bias=child.bias is not None)\n",
    "\n",
    "            # Inflate weights\n",
    "            linear2d_weights = child.weight.data # Shape [out_features, in_features_2d]\n",
    "\n",
    "            # Reshape 2D weights to [out_features, in_features_2d, 1] (add a temporal dimension)\n",
    "            inflated_weights = linear2d_weights.unsqueeze(2)\n",
    "            # Repeat along the new temporal dimension\n",
    "            inflated_weights = inflated_weights.repeat(1, 1, final_time_dim)\n",
    "            # Reshape to match the 3D linear layer's expected shape [out_features, inflated_in_features]\n",
    "            inflated_weights = inflated_weights.view(child.out_features, inflated_in_features)\n",
    "\n",
    "            linear3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "            # Copy bias if it exists\n",
    "            if child.bias is not None:\n",
    "                linear3d.bias.data.copy_(child.bias.data)\n",
    "\n",
    "            classifier_3d.add_module(name, linear3d)\n",
    "\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in classifier: {name} ({type(child)})\")\n",
    "            pass # DenseNet's classifier is typically just a Linear layer\n",
    "\n",
    "    # Combine features and classifier into a new Sequential model\n",
    "    # We need to manually handle the final pooling and flatten step in the forward pass\n",
    "    # like in the previous I3D code, as Sequential doesn't do this automatically.\n",
    "    # So we'll create a custom module for the final model.\n",
    "\n",
    "    class InflatedDenseNetModel(nn.Module):\n",
    "        def __init__(self, features_3d, classifier_3d, final_time_dim, final_layer_nb):\n",
    "            super().__init__()\n",
    "            self.features = features_3d\n",
    "            self.classifier = classifier_3d\n",
    "            self.final_time_dim = final_time_dim\n",
    "            self.final_layer_nb = final_layer_nb # This is the number of channels before global pooling\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            # Apply ReLU after features (matches typical DenseNet flow before classification)\n",
    "            x = nn.functional.relu(x)\n",
    "            # Global spatial average pooling. Kernel size matches expected output spatial dims.\n",
    "            # Assuming 7x7 spatial dims before classification for DenseNet121 on ImageNet size inputs\n",
    "            spatial_kernel_h = x.shape[-2]\n",
    "            spatial_kernel_w = x.shape[-1]\n",
    "            x = nn.functional.avg_pool3d(x, kernel_size=(1, spatial_kernel_h, spatial_kernel_w))\n",
    "            # Flatten for classifier\n",
    "            # Original shape: [batch, channels, depth, 1, 1] after spatial pooling\n",
    "            # Permute to [batch, depth, channels, 1, 1]\n",
    "            x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "            # View to [batch, depth * channels]\n",
    "            x = x.view(-1, self.final_time_dim * self.final_layer_nb)\n",
    "            # Pass through classifier\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    # Get the number of channels before the original 2D classifier\n",
    "    final_layer_nb = densenet2d.classifier.in_features\n",
    "\n",
    "    return InflatedDenseNetModel(features_3d, classifier_3d, final_time_dim, final_layer_nb)\n",
    "\n",
    "\n",
    "# --- Testing the Inflation ---\n",
    "\n",
    "import warnings\n",
    "\n",
    "# a. Take a 2D pretrained DenseNet121 model\n",
    "model_2d = models.densenet121(pretrained=True)\n",
    "print(\"Loaded 2D DenseNet121 model.\")\n",
    "\n",
    "# Expected number of frames in your video input\n",
    "input_frame_nb = 16\n",
    "# Temporal kernel depth for inflating 3x3 convs\n",
    "conv_k_depth = 3\n",
    "# Temporal stride for pooling in transition blocks\n",
    "pool_t_stride = 2\n",
    "\n",
    "# Inflate the model\n",
    "i3d_densenet_model = inflate_densenet121(\n",
    "    model_2d,\n",
    "    frame_nb=input_frame_nb,\n",
    "    conv_kernel_depth=conv_k_depth,\n",
    "    temporal_pool_stride=pool_t_stride\n",
    ")\n",
    "\n",
    "print(f\"\\nInflated 3D DenseNet model created for {input_frame_nb} frames.\")\n",
    "# Print the structure of the inflated model (optional, can be long)\n",
    "# print(i3d_densenet_model)\n",
    "\n",
    "\n",
    "# Create a dummy 3D input tensor\n",
    "# Shape: [batch_size, channels, depth, height, width]\n",
    "batch_size = 2\n",
    "channels = 1\n",
    "input_height = 224\n",
    "input_width = 224\n",
    "\n",
    "dummy_input_3d = torch.randn(batch_size, channels, input_frame_nb, input_height, input_width)\n",
    "print(\"\\nDummy input shape:\", dummy_input_3d.shape)\n",
    "\n",
    "\n",
    "# Perform a forward pass with the dummy input\n",
    "try:\n",
    "    i3d_densenet_model.eval() # Set model to evaluation mode (disables dropout, uses running stats for BN)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        output = i3d_densenet_model(dummy_input_3d)\n",
    "\n",
    "    print(\"\\nForward pass successful!\")\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # The output shape should be [batch_size, num_classes]\n",
    "    # For DenseNet121 pre-trained on ImageNet, num_classes is 1000\n",
    "    print(\"Expected output shape:\", (batch_size, 1000))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nForward pass failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9694849b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'densenet2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 473\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# --- Corrected Classifier Inflation in inflate_densenet121 ---\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# (This part was already corrected in the previous response, but including it for completeness)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# This part goes *after* the features inflation loop in inflate_densenet121\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Get the original 2D Linear classifier module directly\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m original_linear_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mdensenet2d\u001b[49m\u001b[38;5;241m.\u001b[39mclassifier\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(original_linear_classifier, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m    476\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected original classifier to be nn.Linear, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(original_linear_classifier)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'densenet2d' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33e8d3ca",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5771af49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3ae840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and inflated successfully.\n",
      "\n",
      "Processing region: Tibia\n",
      "  Processing volume: tibia.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:115: UserWarning: NIfTI volume depth (512) does not match frame_nb (64) used during model inflation. This might cause issues or requires padding/cropping.\n",
      "  warnings.warn(f\"NIfTI volume depth ({current_volume_depth}) does not match frame_nb ({frame_nb}) \"\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:153: UserWarning: Invalid target layer format: third_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/216718156.py:153: UserWarning: Invalid target layer format: fifth_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded volume shape: (512, 512, 216), Prepared tensor shape: torch.Size([1, 1, 512, 512, 216])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib  # For loading NIfTI images\n",
    "import numpy as np\n",
    "# Assuming your previous code is in a file named 'i3d_densenet_inflation.py'\n",
    "# from i3d_densenet_inflation import inflate_densenet121\n",
    "import torchvision.models as models # Needed to load the 2D model\n",
    "\n",
    "\n",
    "# --- (Include the inflate_densenet121 function and its helpers from previous responses) ---\n",
    "# You should copy and paste the inflate_conv, inflate_batch_norm, inflate_relu,\n",
    "# inflate_pool, InflatedDenseLayer, InflatedTransition, and inflate_densenet121\n",
    "# definitions here or import them from your file.\n",
    "# For brevity, I'll assume they are defined above this code block.\n",
    "\n",
    "# --- Helper function to find specific convolution layers ---\n",
    "\n",
    "def find_convolution_layers(module):\n",
    "    \"\"\"Recursively finds all Conv3d layers within a PyTorch module.\"\"\"\n",
    "    conv_layers = []\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Conv3d):\n",
    "            conv_layers.append((name, child))\n",
    "        else:\n",
    "            # Recurse into submodules\n",
    "            conv_layers.extend(find_convolution_layers(child))\n",
    "    return conv_layers\n",
    "\n",
    "# --- Main Feature Extraction Function ---\n",
    "\n",
    "def extract_features_from_volume(model_3d, volume_path, frame_nb, target_layers=['last', 'third_last', 'fifth_last']):\n",
    "    \"\"\"\n",
    "    Loads a NIfTI volume, runs it through the 3D CNN, and extracts feature maps\n",
    "    from specified convolution layers, applying Global Average Pooling.\n",
    "\n",
    "    Args:\n",
    "        model_3d (torch.nn.Module): The inflated 3D DenseNet model.\n",
    "        volume_path (str): Path to the NIfTI file (.nii or .nii.gz).\n",
    "        frame_nb (int): The expected number of frames/slices in the input volume.\n",
    "                         This should match the `frame_nb` used during inflation.\n",
    "        target_layers (list): List of strings specifying which convolution layers\n",
    "                              to extract features from. Options: 'last', 'second_last',\n",
    "                              'third_last', etc., or a specific layer name (though names\n",
    "                              can be complex in nested structures).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the target layer names/indices\n",
    "              and values are the corresponding N-dimensional feature vectors\n",
    "              after Global Average Pooling. Returns an empty dictionary if no\n",
    "              valid layers are specified or found.\n",
    "    Raises:\n",
    "        FileNotFoundError: If the volume_path does not exist.\n",
    "        ValueError: If the volume's temporal dimension does not match frame_nb\n",
    "                    or if specified target layers are not found.\n",
    "        RuntimeError: If a forward pass fails.\n",
    "    \"\"\"\n",
    "    # 1. Load NIfTI image\n",
    "    if not os.path.exists(volume_path):\n",
    "        raise FileNotFoundError(f\"Volume not found at: {volume_path}\")\n",
    "\n",
    "    img = nib.load(volume_path)\n",
    "    data = img.get_fdata()\n",
    "    # Ensure data is float32, which is typical for model inputs\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    # NIfTI data often has shape [depth, height, width] or similar spatial dims\n",
    "    # We need to prepare it for the PyTorch model: [batch_size, channels, depth, height, width]\n",
    "    # Assuming your NIfTI is grayscale (1 channel) or already has channel dim\n",
    "    # If grayscale [depth, height, width], add a channel dimension at position 0 or 1\n",
    "    # If it's [depth, height, width] and represents time series, depth is time.\n",
    "    # Let's assume input NIfTI is [depth, height, width] (spatial) and we add a channel and batch dim.\n",
    "    # Or, if it's a video/sequence [time, depth, height, width], we need to reorder.\n",
    "    # Based on the model's expected input [batch_size, channels, depth, height, width],\n",
    "    # let's assume the NIfTI data is [depth, height, width] and represents *one time step*,\n",
    "    # and you are providing multiple volumes or slices that form the 'frame_nb' dimension.\n",
    "    # A more realistic approach for medical imaging might be [slices, height, width]\n",
    "    # where 'slices' corresponds to the depth dimension in the 3D convolution.\n",
    "    # Let's assume the NIfTI is [depth, height, width] and treat 'depth' as the temporal dim.\n",
    "    # We need to ensure it matches the expected `frame_nb`.\n",
    "\n",
    "    # Adjust this reshaping based on your actual NIfTI data format and how you want to map it to [channels, depth, height, width]\n",
    "    # Assuming NIfTI is [Depth, Height, Width] -> map to [1, Depth, Height, Width] (1 channel grayscale)\n",
    "    # Then add batch dim -> [1, 1, Depth, Height, Width]\n",
    "    # And finally ensure Depth matches frame_nb.\n",
    "    # A common format might be [Depth, Height, Width, Channels] for color or multi-spectral.\n",
    "    # Let's assume input NIfTI data has shape [SpatialDepth, SpatialHeight, SpatialWidth]\n",
    "    # and you want to stack `frame_nb` such volumes or slices along the first dim to create the temporal dimension.\n",
    "    # Or, maybe your NIfTI itself is [Time, SpatialDepth, SpatialHeight, SpatialWidth].\n",
    "\n",
    "    # Let's simplify and assume the loaded 'data' from NIfTI is already in\n",
    "    # a format where you can directly extract `frame_nb` slices/volumes that\n",
    "    # when stacked or selected, form the input `[channels, depth, height, width]` tensor.\n",
    "    # For a single 3D volume treated as the 'depth' dimension in the CNN:\n",
    "    # Assume data is [Depth, Height, Width]. We want [1, Depth, Height, Width] for 1 channel.\n",
    "    # The model expects [batch_size, channels, depth, height, width]\n",
    "    # So, input_tensor will be [1, 1, Depth, Height, Width] where Depth = frame_nb\n",
    "\n",
    "    # Let's refine the input tensor preparation:\n",
    "    # Assuming your NIfTI data is a single 3D volume of shape [D, H, W]\n",
    "    # And you want to treat D as the 'depth' dimension for the 3D CNN.\n",
    "    # Your CNN expects [batch_size, channels, depth, height, width].\n",
    "    # So, for a single volume, batch_size=1, channels=1 (grayscale), depth=D, height=H, width=W.\n",
    "    # The `frame_nb` passed to this function should be D.\n",
    "\n",
    "    input_data_shape = data.shape\n",
    "    if len(input_data_shape) != 3:\n",
    "        # Handle potential 4D NIfTI data if needed (e.g., [Depth, H, W, Channels] or [Time, D, H, W])\n",
    "        # This part requires knowing your specific NIfTI format\n",
    "        raise ValueError(f\"Expected 3D NIfTI data [D, H, W], but got shape {input_data_shape}. \"\n",
    "                         \"Adjust input processing based on your NIfTI format.\")\n",
    "\n",
    "    current_volume_depth = input_data_shape[0] # Assuming depth is the first dim\n",
    "\n",
    "    if current_volume_depth != frame_nb:\n",
    "         warnings.warn(f\"NIfTI volume depth ({current_volume_depth}) does not match frame_nb ({frame_nb}) \"\n",
    "                       \"used during model inflation. This might cause issues or requires padding/cropping.\")\n",
    "         # You might need padding or cropping here if the volume depth doesn't match frame_nb\n",
    "         # For simplicity, we'll proceed but be aware this is a potential problem.\n",
    "         # If your model expects a fixed frame_nb, you *must* match it.\n",
    "         # If your model can handle variable temporal length (less common for fixed-size inputs),\n",
    "         # you need to adjust the model's final layer logic.\n",
    "\n",
    "    # Add channel and batch dimensions\n",
    "    # data shape [D, H, W] -> tensor shape [1, 1, D, H, W]\n",
    "    input_tensor = torch.from_numpy(data).unsqueeze(0).unsqueeze(0) # Add channel and batch dim\n",
    "\n",
    "    print(f\"Loaded volume shape: {input_data_shape}, Prepared tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "\n",
    "    # Find all Conv3d layers and their names\n",
    "    all_conv_layers = find_convolution_layers(model_3d)\n",
    "    if not all_conv_layers:\n",
    "        raise ValueError(\"No Conv3d layers found in the model's features.\")\n",
    "\n",
    "    # Map target layer names/indices to actual layers\n",
    "    layers_to_extract = {}\n",
    "    num_conv_layers = len(all_conv_layers)\n",
    "\n",
    "    for target in target_layers:\n",
    "        if target == 'last':\n",
    "            if num_conv_layers >= 1:\n",
    "                layers_to_extract['last'] = all_conv_layers[-1]\n",
    "            else:\n",
    "                warnings.warn(\"Target 'last' specified, but no Conv3d layers found.\")\n",
    "        elif target.endswith('_last'):\n",
    "            try:\n",
    "                index = int(target.split('_')[0])\n",
    "                if index > 0 and num_conv_layers >= index:\n",
    "                    layers_to_extract[target] = all_conv_layers[-index]\n",
    "                else:\n",
    "                    warnings.warn(f\"Target '{target}' specified, but not enough Conv3d layers ({num_conv_layers}) found.\")\n",
    "            except ValueError:\n",
    "                warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n",
    "        # Add support for specific layer names if needed\n",
    "        # elif isinstance(target, str):\n",
    "        #    found = False\n",
    "        #    for layer_name, layer_module in all_conv_layers:\n",
    "        #        if layer_name == target:\n",
    "        #             layers_to_extract[target] = (layer_name, layer_module)\n",
    "        #             found = True\n",
    "        #             break\n",
    "        #    if not found:\n",
    "        #        warnings.warn(f\"Target layer '{target}' not found by name.\")\n",
    "        else:\n",
    "            warnings.warn(f\"Invalid target layer specification: {target}.\")\n",
    "\n",
    "    if not layers_to_extract:\n",
    "        warnings.warn(\"No valid target layers specified or found for feature extraction.\")\n",
    "        return {}\n",
    "\n",
    "    # Hook to capture intermediate feature maps\n",
    "    feature_maps = {}\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # We identify the layer by its module object reference\n",
    "        layer_info = None\n",
    "        for name, layer_module in all_conv_layers:\n",
    "             if layer_module is module:\n",
    "                 layer_info = (name, layer_module)\n",
    "                 break\n",
    "        if layer_info:\n",
    "             # Store the output tensor. Clone it to avoid issues if the tensor is modified later.\n",
    "             feature_maps[layer_info] = output.clone()\n",
    "\n",
    "    # Register hooks on the target layers\n",
    "    for _, (layer_name, layer_module) in layers_to_extract.items():\n",
    "         # Find the actual module instance in the model's features sequential\n",
    "         # This part can be tricky with deeply nested Sequential/ModuleList structures.\n",
    "         # A simpler approach is to use the layer_module object directly from all_conv_layers\n",
    "         # and register the hook on that specific instance.\n",
    "         hooks.append(layer_module.register_forward_hook(hook_fn))\n",
    "\n",
    "\n",
    "    # Set model to evaluation mode and disable gradients\n",
    "    model_3d.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the forward pass\n",
    "        # The forward pass will execute the hooks when the target layers are reached.\n",
    "        try:\n",
    "            # We run the full forward pass, but only care about the captured feature maps\n",
    "            # The final output of the model is not needed for feature extraction\n",
    "            _ = model_3d(input_tensor)\n",
    "        except Exception as e:\n",
    "            # Clean up hooks even if forward pass fails\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            raise RuntimeError(f\"Forward pass failed during feature extraction: {e}\")\n",
    "\n",
    "\n",
    "    # Clean up the hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # 3. Apply Global Average Pooling (GAP) to extracted feature maps\n",
    "    extracted_features = {}\n",
    "    for (layer_name, layer_module), feature_map in feature_maps.items():\n",
    "        # Feature map shape is [batch_size, channels, depth, height, width]\n",
    "        # GAP should pool over depth, height, and width dimensions\n",
    "        # For a single volume, batch_size is 1.\n",
    "        # Output shape after GAP should be [batch_size, channels]\n",
    "\n",
    "        # Ensure feature_map has at least 3 spatial/temporal dimensions to pool over\n",
    "        if feature_map.dim() < 5:\n",
    "             warnings.warn(f\"Feature map for layer {layer_name} has unexpected dimensions {feature_map.shape}. Skipping GAP.\")\n",
    "             continue # Skip GAP if shape is not as expected\n",
    "\n",
    "        # Apply 3D Global Average Pooling\n",
    "        # Reduce dimensions 2, 3, and 4 (depth, height, width) to size 1\n",
    "        pooled_features = nn.functional.avg_pool3d(feature_map, kernel_size=feature_map.size()[2:])\n",
    "\n",
    "        # The shape is now [batch_size, channels, 1, 1, 1]. Squeeze out the singleton dimensions.\n",
    "        feature_vector = pooled_features.squeeze(-1).squeeze(-1).squeeze(-1) # Shape [batch_size, channels]\n",
    "\n",
    "        # For a single volume (batch_size=1), the shape is [1, channels]. Squeeze the batch dim.\n",
    "        if feature_vector.shape[0] == 1:\n",
    "             feature_vector = feature_vector.squeeze(0) # Shape [channels]\n",
    "\n",
    "        extracted_features[(layer_name, type(layer_module).__name__)] = feature_vector\n",
    "\n",
    "\n",
    "    return extracted_features\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "import os # Needed for os.path.exists\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming your NIfTI files are organized by region\n",
    "    nifti_dir = 'segmented_regions' # Change this to your directory\n",
    "    regions = ['Tibia', 'Femur', 'Background'] # Or whatever your regions are\n",
    "\n",
    "    # --- Load and Inflate the Model ---\n",
    "    try:\n",
    "        model_2d = models.densenet121(pretrained=True)\n",
    "        input_frame_nb_for_inflation = 64 # Example: Inflate for volumes with 64 slices/frames\n",
    "        # Make sure this matches the actual depth of your NIfTI volumes\n",
    "        i3d_densenet_model = inflate_densenet121(model_2d, frame_nb=input_frame_nb_for_inflation)\n",
    "        print(\"Model loaded and inflated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or inflating model: {e}\")\n",
    "        exit() # Exit if model setup fails\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate through regions and potentially multiple volumes per region\n",
    "    for region in regions:\n",
    "        print(f\"\\nProcessing region: {region}\")\n",
    "        region_features = []\n",
    "        #region_dir = os.path.join(nifti_dir, region)\n",
    "\n",
    "        # Assuming NIfTI files are directly in the region directory\n",
    "        volume_files = [f for f in os.listdir(nifti_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "\n",
    "        if not volume_files:\n",
    "            print(f\"No NIfTI files found in {region_dir}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for volume_file in volume_files:\n",
    "            volume_path = os.path.join(nifti_dir, volume_file)\n",
    "            print(f\"  Processing volume: {volume_file}\")\n",
    "\n",
    "            try:\n",
    "                # Define the layers you want to extract from.\n",
    "                # This depends on the specific structure of the inflated DenseNet.\n",
    "                # You might need to inspect the printed model structure to figure out which\n",
    "                # indices correspond to the last, third-last, fifth-last *convolution* layers.\n",
    "                # The 'last', 'third_last', etc. logic in find_convolution_layers handles this based on indices.\n",
    "                layers_to_get = ['last', 'third_last', 'fifth_last'] # Example targets\n",
    "\n",
    "                extracted_feats = extract_features_from_volume(\n",
    "                    i3d_densenet_model,\n",
    "                    volume_path,\n",
    "                    frame_nb=input_frame_nb_for_inflation, # Must match inflation frame_nb\n",
    "                    target_layers=layers_to_get\n",
    "                )\n",
    "                region_features.append((volume_file, extracted_feats))\n",
    "                print(f\"    Extracted features for layers: {list(extracted_feats.keys())}\")\n",
    "                # Example: print the shape of a feature vector\n",
    "                # if extracted_feats:\n",
    "                #      first_key = list(extracted_feats.keys())[0]\n",
    "                #      print(f\"    Shape of features from {first_key}: {extracted_feats[first_key].shape}\")\n",
    "\n",
    "\n",
    "            except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "                print(f\"    Error processing volume {volume_file}: {e}\")\n",
    "                continue # Continue to the next volume even if one fails\n",
    "\n",
    "        all_extracted_features[region] = region_features\n",
    "\n",
    "    print(\"\\n--- Feature Extraction Complete ---\")\n",
    "    # all_extracted_features is a dictionary:\n",
    "    # {\n",
    "    #   'Tibia': [(volume_file1, {layer1: features, layer2: features, ...}), (volume_file2, {...}), ...],\n",
    "    #   'Femur': [...],\n",
    "    #   'Background': [...]\n",
    "    # }\n",
    "\n",
    "    # You can now use the 'all_extracted_features' dictionary for downstream tasks\n",
    "    # (e.g., training a classifier on these feature vectors)\n",
    "    # Example: Access features for the first volume of the Tibia region, from the 'last' convolution layer\n",
    "    # if 'Tibia' in all_extracted_features and all_extracted_features['Tibia']:\n",
    "    #     first_tibia_volume_features = all_extracted_features['Tibia'][0][1]\n",
    "    #     if ('last', 'Conv3d') in first_tibia_volume_features: # Check if 'last' was extracted and is Conv3d\n",
    "    #          last_conv_features = first_tibia_volume_features[('last', 'Conv3d')]\n",
    "    #          print(\"\\nExample: Features from last Conv3d of first Tibia volume:\")\n",
    "    #          print(last_conv_features)\n",
    "    #          print(\"Shape:\", last_conv_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8fe6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and inflated successfully.\n",
      "  Loaded original volume data with shape: (512, 512, 216)\n",
      "      Processing region: tibia\n",
      "        Preprocessed tensor shape: torch.Size([1, 1, 16, 101, 216])\n",
      "  Error processing volume:       Forward pass failed for region tibia: Given groups=1, weight of size [64, 3, 7, 7, 7], expected input[1, 1, 16, 101, 216] to have 3 channels, but got 1 channels instead\n",
      "\n",
      "--- Region Feature Extraction Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:96: UserWarning:         Cropped region depth (116) does not match target frame_nb (16). Applying padding/cropping.\n",
      "  warnings.warn(f\"        Cropped region depth ({current_region_depth}) does not match target frame_nb ({target_depth}). Applying padding/cropping.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:159: UserWarning: Invalid target layer format: third_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n",
      "/var/folders/kb/f1k4_th17h992yvkrm2v50d40000gn/T/ipykernel_3536/4205872307.py:159: UserWarning: Invalid target layer format: fifth_last. Use 'last', 'second_last', etc.\n",
      "  warnings.warn(f\"Invalid target layer format: {target}. Use 'last', 'second_last', etc.\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f5170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11787ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Background_sample_volume.nii.gz',\n",
       "  {('conv2',\n",
       "    'Conv3d'): tensor([ 0.0051,  0.0122, -0.0055, -0.0065,  0.0012, -0.0178, -0.0320, -0.0056,\n",
       "           -0.0361, -0.0274, -0.0244, -0.0153, -0.0003, -0.0026, -0.0136,  0.0120,\n",
       "            0.0023,  0.0165, -0.0097, -0.0062, -0.0051,  0.0130, -0.0266,  0.0109,\n",
       "           -0.0180, -0.0190, -0.0017,  0.0228, -0.0170,  0.0546,  0.0062,  0.0015])})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ed14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
