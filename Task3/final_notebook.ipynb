{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "b6a2a",
    "execution": {
     "iopub.execute_input": "2025-06-09T10:20:08.649837Z",
     "iopub.status.busy": "2025-06-09T10:20:08.649661Z",
     "iopub.status.idle": "2025-06-09T10:20:09.511166Z",
     "shell.execute_reply": "2025-06-09T10:20:09.510389Z",
     "shell.execute_reply.started": "2025-06-09T10:20:08.649814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellUniqueIdByVincent": "03ccb",
    "execution": {
     "iopub.execute_input": "2025-06-09T10:20:33.159062Z",
     "iopub.status.busy": "2025-06-09T10:20:33.158789Z",
     "iopub.status.idle": "2025-06-09T10:20:33.162901Z",
     "shell.execute_reply": "2025-06-09T10:20:33.162078Z",
     "shell.execute_reply.started": "2025-06-09T10:20:33.159040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ct_file = \"/kaggle/input/left-knee/left_knee.nii\"\n",
    "mask_file = \"/kaggle/input/left-knee/bone_segmentation_task1_1.nii\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "cec40",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize(image_data,name):\n",
    "    # --- Choose slices to visualize ---\n",
    "    num_slices = 9\n",
    "    slice_indices = np.linspace(0, image_data.shape[2] - 1, num_slices, dtype=int)\n",
    "\n",
    "    # --- Plot slices ---\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        axes[i].imshow(image_data[:, :, slice_idx], cmap='gray')\n",
    "        axes[i].set_title(f\"Slice {slice_idx}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.savefig(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_nifti(file_path):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file and return the data array and affine.\n",
    "    \"\"\"\n",
    "    nifti = nib.load(file_path)\n",
    "    data = nifti.get_fdata()\n",
    "    affine = nifti.affine\n",
    "    return data, affine\n",
    "\n",
    "\n",
    "def save_nifti(data, affine, output_path):\n",
    "    \"\"\"\n",
    "    Save a NumPy array as a NIfTI file.\n",
    "    \"\"\"\n",
    "    nifti = nib.Nifti1Image(data.astype(np.float32), affine)\n",
    "    nib.save(nifti, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "497a0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_data,_=load_nifti(ct_file)\n",
    "visualize(image_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "84afb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def segment_knee_regions(ct_path, mask_path, output_dir):\n",
    "    \"\"\"\n",
    "    Segment Tibia, Femur, and Background from a CT volume using the mask.\n",
    "    \"\"\"\n",
    "    # Load the CT scan and segmentation mask\n",
    "    ct_data, ct_affine = load_nifti(ct_path)\n",
    "    mask_data, _ = load_nifti(mask_path)\n",
    "\n",
    "    # Define region labels (change if your mask uses different values)\n",
    "    TIBIA_LABEL = 1\n",
    "    FEMUR_LABEL = 2\n",
    "    BACKGROUND_LABEL = 0\n",
    "\n",
    "    # Generate binary masks\n",
    "    tibia_mask = (mask_data == TIBIA_LABEL)\n",
    "    femur_mask = (mask_data == FEMUR_LABEL)\n",
    "    background_mask = (mask_data == BACKGROUND_LABEL)\n",
    "\n",
    "    # Apply masks to CT data\n",
    "    tibia_volume = ct_data * tibia_mask\n",
    "    femur_volume = ct_data * femur_mask\n",
    "    background_volume = ct_data * background_mask\n",
    "\n",
    "    # Save each region as a new .nii.gz file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_nifti(tibia_volume, ct_affine, os.path.join(output_dir, \"tibia_volume.nii.gz\"))\n",
    "    save_nifti(femur_volume, ct_affine, os.path.join(output_dir, \"femur_volume.nii.gz\"))\n",
    "    save_nifti(background_volume, ct_affine, os.path.join(output_dir, \"background_volume.nii.gz\"))\n",
    "\n",
    "    print(\"Segmentation done. Files saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "b7afe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "segment_knee_regions(ct_file, mask_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e1b2e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def overlay_mask(image_slice, mask_slice, alpha=0.4, cmap_img='gray', cmap_mask='jet'):\n",
    "    \"\"\"\n",
    "    Overlay a segmentation mask on a single image slice.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_slice: 2D numpy array of the image slice.\n",
    "    - mask_slice: 2D numpy array of the mask slice (same size as image_slice).\n",
    "    - alpha: float, transparency of the mask overlay.\n",
    "    - cmap_img: str, colormap for the image.\n",
    "    - cmap_mask: str, colormap for the mask.\n",
    "    \"\"\"\n",
    "    plt.imshow(image_slice, cmap=cmap_img)\n",
    "    if mask_slice is not None and np.any(mask_slice):\n",
    "        plt.imshow(mask_slice, cmap=cmap_mask, alpha=alpha)\n",
    "    plt.axis('off')\n",
    "\n",
    "def visualize_nifti_with_mask(image_path, mask_path=None, num_slices=9, axis=2):\n",
    "    \"\"\"\n",
    "    Visualize NIfTI image slices with optional mask overlays.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image .nii.gz file.\n",
    "    - mask_path: str or None, path to the mask .nii.gz file (optional).\n",
    "    - num_slices: int, number of slices to visualize.\n",
    "    - axis: int, axis along which to slice (0=sagittal, 1=coronal, 2=axial).\n",
    "    \"\"\"\n",
    "    # Load image and optional mask\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    assert mask is None or image.shape == mask.shape, \"Image and mask must have the same shape\"\n",
    "\n",
    "    # Select slices evenly along the chosen axis\n",
    "    slice_indices = np.linspace(0, image.shape[axis] - 1, num_slices, dtype=int)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(slice_indices):\n",
    "        ax = axes[i]\n",
    "        plt.sca(ax)\n",
    "\n",
    "        if axis == 0:\n",
    "            img_slice = image[idx, :, :]\n",
    "            msk_slice = mask[idx, :, :] if mask is not None else None\n",
    "        elif axis == 1:\n",
    "            img_slice = image[:, idx, :]\n",
    "            msk_slice = mask[:, idx, :] if mask is not None else None\n",
    "        else:  # default to axial\n",
    "            img_slice = image[:, :, idx]\n",
    "            msk_slice = mask[:, :, idx] if mask is not None else None\n",
    "\n",
    "        overlay_mask(img_slice, msk_slice)\n",
    "        ax.set_title(f\"Slice {idx}\")\n",
    "    plt.savefig('slices with masks ')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "19ece",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_nifti_with_mask(\n",
    "    ct_file,\n",
    "    mask_file,  # Set to None if you donâ€™t have a mask\n",
    "    num_slices=9,\n",
    "    axis=2  # axial view\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "2a915",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_image_mask_overlay(image_path, mask_path, slice_index, axis=2, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Plot image slice, mask slice, and overlay side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: 3D numpy array (CT volume)\n",
    "    - mask: 3D numpy array (segmentation mask)\n",
    "    - slice_index: int, index of the slice to visualize\n",
    "    - axis: int, slicing axis (0=sagittal, 1=coronal, 2=axial)\n",
    "    - alpha: float, transparency for overlay\n",
    "    \"\"\"\n",
    "    image, ct_affine = load_nifti(image_path)\n",
    "    mask, _ = load_nifti(mask_path)\n",
    "    # Extract slices\n",
    "    if axis == 0:\n",
    "        img_slice = image[slice_index, :, :]\n",
    "        mask_slice = mask[slice_index, :, :]\n",
    "    elif axis == 1:\n",
    "        img_slice = image[:, slice_index, :]\n",
    "        mask_slice = mask[:, slice_index, :]\n",
    "    else:\n",
    "        img_slice = image[:, :, slice_index]\n",
    "        mask_slice = mask[:, :, slice_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].imshow(img_slice, cmap='gray')\n",
    "    axs[0].set_title(f'Image Slice {slice_index}')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(mask_slice, cmap='jet')\n",
    "    axs[1].set_title(f'Mask Slice {slice_index}')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(img_slice, cmap='gray')\n",
    "    axs[2].imshow(mask_slice, cmap='jet', alpha=alpha)\n",
    "    axs[2].set_title('Overlay')\n",
    "    axs[2].axis('off')\n",
    "    plt.savefig('slice_108_with_maska')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "15f9c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "slice_index = 108  \n",
    "axis = 2  # Axial view\n",
    "\n",
    "plot_image_mask_overlay(ct_file, mask_file, slice_index, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "60135",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellUniqueIdByVincent": "15c82",
    "execution": {
     "iopub.execute_input": "2025-06-09T11:18:24.028981Z",
     "iopub.status.busy": "2025-06-09T11:18:24.028669Z",
     "iopub.status.idle": "2025-06-09T11:18:24.614256Z",
     "shell.execute_reply": "2025-06-09T11:18:24.613508Z",
     "shell.execute_reply.started": "2025-06-09T11:18:24.028958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2D DenseNet121 model.\n",
      "\n",
      "Inflated 3D DenseNet model created for 512 frames.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "# --- Helper Functions for Inflation ---\n",
    "\n",
    "def inflate_conv(conv2d, kernel_depth):\n",
    "    \"\"\"Inflates a Conv2d layer to a Conv3d layer by repeating weights.\"\"\"\n",
    "    if conv2d.in_channels % conv2d.groups != 0 or conv2d.out_channels % conv2d.groups != 0:\n",
    "         raise ValueError(\"Inflating grouped convolutions is not straightforward with simple repetition.\")\n",
    "\n",
    "    # Get 2D parameters\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size_2d = conv2d.kernel_size\n",
    "    stride_2d = conv2d.stride\n",
    "    padding_2d = conv2d.padding\n",
    "    dilation_2d = conv2d.dilation\n",
    "    groups = conv2d.groups\n",
    "    bias = conv2d.bias is not None\n",
    "\n",
    "    # Create 3D convolution parameters\n",
    "    # Temporal kernel size\n",
    "    kernel_size_3d = (kernel_depth, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    \n",
    "    # Typically stride 1 in time for simple inflation unless downsampling is desired\n",
    "    # Based on the previous I3D code structure, transitions downsample spatially,\n",
    "    # and the initial conv might downsample spatially. Temporal downsampling is handled\n",
    "    # by the pooling layers in transition blocks.\n",
    "    # So we'll use a temporal stride of 1 here for convolution inflation.\n",
    "    stride_3d = (1, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding to keep temporal dimension size\n",
    "    padding_3d = (kernel_depth // 2, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1]) # Dilation usually 1 in time\n",
    "\n",
    "    # Create 3D Conv layer\n",
    "    conv3d = nn.Conv3d(in_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                       dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "    # Inflate weights (replication and scaling)\n",
    "    conv2d_weights = conv2d.weight.data #  Shape [out_channels, 3, H, W]\n",
    "    #averaged_weights = conv2d_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "\n",
    "                 # Now repeat this 1-channel weight along the temporal dimension\n",
    "    inflated_weights = conv2d_weights.unsqueeze(2).repeat(1, 1, kernel_depth, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize by dividing by the depth (as per requirement ii)\n",
    "    if kernel_depth > 0: # Avoid division by zero if somehow kernel_depth is 0\n",
    "        inflated_weights = inflated_weights / kernel_depth\n",
    "\n",
    "    # Copy inflated weights to the 3D Conv layer\n",
    "    conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "    # Copy bias if it exists\n",
    "    if bias:\n",
    "        conv3d.bias.data.copy_(conv2d.bias.data)\n",
    "\n",
    "    return conv3d\n",
    "\n",
    "def inflate_batch_norm(bn2d):\n",
    "    \"\"\"Inflates a BatchNorm2d layer to a BatchNorm3d layer.\"\"\"\n",
    "    bn3d = nn.BatchNorm3d(bn2d.num_features)\n",
    "    # Copy parameters and running statistics\n",
    "    bn3d.weight.data.copy_(bn2d.weight.data)\n",
    "    bn3d.bias.data.copy_(bn2d.bias.data)\n",
    "    bn3d.running_mean.copy_(bn2d.running_mean)\n",
    "    bn3d.running_var.copy_(bn2d.running_var)\n",
    "    # bn3d.num_batches_tracked.copy_(bn2d.num_batches_tracked) # Copy if your BN uses this\n",
    "    return bn3d\n",
    "\n",
    "def inflate_relu(relu2d):\n",
    "    \"\"\"Returns a ReLU layer (same for 2D and 3D).\"\"\"\n",
    "    return nn.ReLU(inplace=relu2d.inplace)\n",
    "\n",
    "def inflate_pool(pool2d, temporal_stride=1):\n",
    "    \"\"\"Inflates a Pooling layer to a 3D Pooling layer.\"\"\"\n",
    "    # Get 2D parameters\n",
    "    kernel_size_2d = pool2d.kernel_size\n",
    "    stride_2d = pool2d.stride\n",
    "    padding_2d = pool2d.padding\n",
    "    dilation_2d = pool2d.dilation if hasattr(pool2d, 'dilation') else 1 # MaxPool2d has dilation\n",
    "    return_indices = pool2d.return_indices if hasattr(pool2d, 'return_indices') else False # MaxPool2d has this\n",
    "    ceil_mode = pool2d.ceil_mode if hasattr(pool2d, 'ceil_mode') else False # Pool2d has this\n",
    "\n",
    "    # Ensure kernel_size, stride, padding are tuples for consistency\n",
    "    if not isinstance(kernel_size_2d, tuple): kernel_size_2d = (kernel_size_2d,) * 2\n",
    "    if not isinstance(stride_2d, tuple): stride_2d = (stride_2d,) * 2\n",
    "    if not isinstance(padding_2d, tuple): padding_2d = (padding_2d,) * 2\n",
    "    if not isinstance(dilation_2d, tuple): dilation_2d = (dilation_2d,) * 2\n",
    "\n",
    "\n",
    "    # Create 3D pooling parameters\n",
    "    # Temporal kernel size (1 for no pooling in time by the pool layer itself, temporal_stride handles downsampling)\n",
    "    kernel_size_3d = (1, kernel_size_2d[0], kernel_size_2d[1])\n",
    "    # Temporal stride for downsampling\n",
    "    stride_3d = (temporal_stride, stride_2d[0], stride_2d[1])\n",
    "    # Temporal padding (0 as we don't pool over time)\n",
    "    padding_3d = (0, padding_2d[0], padding_2d[1])\n",
    "    dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "    if isinstance(pool2d, nn.MaxPool2d):\n",
    "        return nn.MaxPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d,\n",
    "                            dilation=dilation_3d, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "    elif isinstance(pool2d, nn.AvgPool2d):\n",
    "        # AvgPool2d also has count_include_pad attribute\n",
    "        count_include_pad = pool2d.count_include_pad if hasattr(pool2d, 'count_include_pad') else True\n",
    "        return nn.AvgPool3d(kernel_size_3d, stride=stride_3d, padding=padding_3d, ceil_mode=ceil_mode,\n",
    "                            count_include_pad=count_include_pad) # Count include pad might need adjustment for 3D?\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {type(pool2d)}\")\n",
    "\n",
    "# --- Inflated DenseNet Components ---\n",
    "\n",
    "class InflatedDenseLayer(nn.Module):\n",
    "    def __init__(self, dense_layer2d, conv_kernel_depth=3):\n",
    "        super(InflatedDenseLayer, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in dense_layer2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d):\n",
    "                # For the bottleneck 1x1 conv, kernel_depth is 1\n",
    "                # For the 3x3 conv, kernel_depth is user-specified (default 3)\n",
    "                self.layers.add_module(name, inflate_conv(child, kernel_depth=child.kernel_size[0] if child.kernel_size != (1,1) else 1))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in DenseLayer: {type(child)}\")\n",
    "        # DenseLayer also has a drop_rate attribute\n",
    "        self.drop_rate = dense_layer2d.drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.layers(x)\n",
    "        if self.drop_rate > 0 and self.training: # Apply dropout only during training\n",
    "            new_features = nn.functional.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        # DenseNet connectivity: concatenate input with new features\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class InflatedTransition(nn.Module):\n",
    "    def __init__(self, transition2d, temporal_pool_stride=2):\n",
    "        super(InflatedTransition, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        for name, child in transition2d.named_children():\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                self.layers.add_module(name, inflate_batch_norm(child))\n",
    "            elif isinstance(child, nn.ReLU):\n",
    "                self.layers.add_module(name, inflate_relu(child))\n",
    "            elif isinstance(child, nn.Conv2d): # This is the 1x1 convolution\n",
    "                 self.layers.add_module(name, inflate_conv(child, kernel_depth=1))\n",
    "            elif isinstance(child, nn.AvgPool2d): # This is the pooling layer for downsampling\n",
    "                self.layers.add_module(name, inflate_pool(child, temporal_stride=temporal_pool_stride))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type in Transition: {type(child)}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Main Inflation Function ---\n",
    "\n",
    "def inflate_densenet121(densenet2d, frame_nb, conv_kernel_depth=3, temporal_pool_stride=2,input_channels=1):\n",
    "    \"\"\"\n",
    "    Inflates a torchvision DenseNet121 model to a 3D model.\n",
    "\n",
    "    Args:\n",
    "        densenet2d (torchvision.models.densenet.DenseNet): The pre-trained 2D DenseNet121 model.\n",
    "        frame_nb (int): The expected number of frames in the input video.\n",
    "        conv_kernel_depth (int): The temporal kernel size to use for inflating 2D convs > 1x1.\n",
    "                                 Default is 3 (3x3x3).\n",
    "        temporal_pool_stride (int): The temporal stride to use for inflating spatial pooling\n",
    "                                    layers in transition blocks. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The inflated 3D DenseNet model.\n",
    "    \"\"\"\n",
    "    # Inflate the features part (contains initial conv, pool, dense blocks, transitions)\n",
    "    features_3d = nn.Sequential()\n",
    "    transition_nb = 0\n",
    "    # Flag to identify the very first convolutional layer\n",
    "    is_first_conv_layer = True\n",
    "    for name, child in densenet2d.features.named_children():\n",
    "        if isinstance(child, nn.Conv2d): # Initial Conv2d\n",
    "            if is_first_conv_layer:\n",
    "                 # --- SPECIAL HANDLING FOR THE VERY FIRST CONV LAYER ---\n",
    "                 # The first Conv2d in DenseNet121 has in_channels=3.\n",
    "                 # We need to change its in_channels to 'input_channels' (e.g., 1)\n",
    "                 # and adapt its weights.\n",
    "\n",
    "                 original_conv2d = child\n",
    "                 original_in_channels = original_conv2d.in_channels # This is 3\n",
    "                 if original_in_channels != 3:\n",
    "                     # This inflation logic assumes the original model starts with 3 channels\n",
    "                     warnings.warn(f\"Expected first Conv2d to have 3 input channels, but got {original_in_channels}. Inflation logic might be incorrect.\")\n",
    "\n",
    "                 # Get original parameters except in_channels\n",
    "                 out_channels = original_conv2d.out_channels\n",
    "                 kernel_size_2d = original_conv2d.kernel_size\n",
    "                 stride_2d = original_conv2d.stride\n",
    "                 padding_2d = original_conv2d.padding\n",
    "                 dilation_2d = original_conv2d.dilation\n",
    "                 groups = original_conv2d.groups\n",
    "                 bias = original_conv2d.bias is not None\n",
    "\n",
    "                 # Create the new Conv3d with the DESIRED input_channels\n",
    "                 # The temporal kernel size for the first layer is typically the spatial kernel size (7x7 -> 7x7x7)\n",
    "                 kernel_depth_3d = kernel_size_2d[0] # Use spatial kernel size for time\n",
    "\n",
    "                 kernel_size_3d = (kernel_depth_3d, kernel_size_2d[0], kernel_size_2d[1])\n",
    "                 stride_3d = (1, stride_2d[0], stride_2d[1]) # Temporal stride 1\n",
    "                 padding_3d = (kernel_depth_3d // 2, padding_2d[0], padding_2d[1])\n",
    "                 dilation_3d = (1, dilation_2d[0], dilation_2d[1])\n",
    "\n",
    "                 first_conv3d = nn.Conv3d(input_channels, out_channels, kernel_size_3d, stride_3d, padding_3d,\n",
    "                                          dilation=dilation_3d, groups=groups, bias=bias)\n",
    "\n",
    "                 # Inflate weights: Original weights shape [out_channels, 3, H, W]\n",
    "                 original_weights = original_conv2d.weight.data\n",
    "\n",
    "                 if input_channels == 1 and original_in_channels == 3:\n",
    "                     # Average the original 3 input channel weights\n",
    "                     adapted_weights_2d = original_weights.mean(dim=1, keepdim=True) # Shape [out_channels, 1, H, W]\n",
    "                 elif input_channels == original_in_channels:\n",
    "                     # Input channels match, just use original weights\n",
    "                     adapted_weights_2d = original_weights\n",
    "                 else:\n",
    "                     # Handle other input channel numbers if needed (e.g., randomly initialize new weights)\n",
    "                     warnings.warn(f\"Handling inflation from {original_in_channels} to {input_channels} channels. \"\n",
    "                                   \"Using averaged weights if original=3, otherwise weights might need custom handling.\")\n",
    "                     if original_in_channels == 3:\n",
    "                         adapted_weights_2d = original_weights.mean(dim=1, keepdim=True).repeat(1, input_channels, 1, 1) # Repeat averaged weight\n",
    "                     else:\n",
    "                         # Fallback: Random initialization for the new layer if sizes don't match\n",
    "                         print(f\"Initializing weights for first Conv3d ({original_in_channels} -> {input_channels}) randomly.\")\n",
    "                         # The first_conv3d layer is already initialized randomly by default, so nothing more to do here.\n",
    "                         # We can just skip the weight copying step.\n",
    "                         adapted_weights_2d = None # Indicate no specific weights to copy\n",
    "\n",
    "\n",
    "                 if adapted_weights_2d is not None:\n",
    "                    # Repeat the adapted 2D weights along the temporal dimension\n",
    "                    # Shape [out_channels, input_channels, 1, H, W] -> [out_channels, input_channels, D, H, W]\n",
    "                    inflated_weights = adapted_weights_2d.unsqueeze(2).repeat(1, 1, kernel_depth_3d, 1, 1)\n",
    "\n",
    "                    # Normalize by dividing by the depth\n",
    "                    if kernel_depth_3d > 0:\n",
    "                         inflated_weights = inflated_weights / kernel_depth_3d\n",
    "\n",
    "                    # Copy inflated weights to the new Conv3d\n",
    "                    first_conv3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "                 if bias:\n",
    "                     first_conv3d.bias.data.copy_(original_conv2d.bias.data)\n",
    "\n",
    "                 features_3d.add_module(name, first_conv3d)\n",
    "                 is_first_conv_layer = False # Mark that the first conv is processed\n",
    "\n",
    "            else:\n",
    "                 # --- STANDARD INFLATION FOR SUBSEQUENT CONV LAYERS ---\n",
    "                 # These layers should use the standard inflate_conv logic\n",
    "                 # Their in_channels will match the out_channels of the preceding 3D layer.\n",
    "                 # We still need to handle the kernel depth for non-1x1 convs.\n",
    "                 temporal_k_depth = child.kernel_size[0] if child.kernel_size != (1,1) else 1\n",
    "                 features_3d.add_module(name, inflate_conv_standard(child, kernel_depth=temporal_k_depth))\n",
    "\n",
    "             # The initial convolution in DenseNet121 is 7x7. Inflate it.\n",
    "            \n",
    "        elif isinstance(child, nn.BatchNorm2d): # Initial BatchNorm\n",
    "             features_3d.add_module(name, inflate_batch_norm(child))\n",
    "        elif isinstance(child, nn.ReLU): # Initial ReLU\n",
    "             features_3d.add_module(name, inflate_relu(child))\n",
    "        elif isinstance(child, nn.MaxPool2d): # Initial MaxPooling\n",
    "             # Initial pool typically reduces spatial dimensions but not temporal\n",
    "             # We'll make the temporal stride 1 here\n",
    "             features_3d.add_module(name, inflate_pool(child, temporal_stride=1))\n",
    "        elif isinstance(child, models.densenet._DenseBlock):\n",
    "             # Inflate the DenseBlock\n",
    "             block_3d = nn.Sequential()\n",
    "             for nested_name, nested_child in child.named_children():\n",
    "                 # Each child in a DenseBlock is a DenseLayer\n",
    "                 assert isinstance(nested_child, models.densenet._DenseLayer)\n",
    "                 block_3d.add_module(nested_name, InflatedDenseLayer(nested_child, conv_kernel_depth=conv_kernel_depth))\n",
    "             features_3d.add_module(name, block_3d)\n",
    "        elif isinstance(child, models.densenet._Transition):\n",
    "             # Inflate the Transition layer\n",
    "             features_3d.add_module(name, InflatedTransition(child, temporal_pool_stride=temporal_pool_stride))\n",
    "             transition_nb += 1\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in features: {name} ({type(child)})\")\n",
    "            pass # Skip layers like OrderedDictWrapper if they appear\n",
    "\n",
    "    # Calculate the final temporal dimension\n",
    "    # Assumes each transition block reduces temporal dimension by temporal_pool_stride\n",
    "    temporal_reduction_factor = int(math.pow(temporal_pool_stride, transition_nb))\n",
    "    final_time_dim = frame_nb // temporal_reduction_factor\n",
    "    if frame_nb % temporal_reduction_factor != 0:\n",
    "         warnings.warn(f\"Input frame_nb ({frame_nb}) is not perfectly divisible by temporal reduction factor ({temporal_reduction_factor}). \"\n",
    "                       \"Final temporal dimension will be floor division result.\")\n",
    "\n",
    "\n",
    "    # Inflate the classifier part\n",
    "    classifier_3d = nn.Sequential()\n",
    "    for name, child in densenet2d.classifier.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # The linear layer input size needs to account for the flattened 3D features\n",
    "            # It's final_time_dim * original_classifier_in_features\n",
    "            original_in_features = child.in_features\n",
    "            inflated_in_features = final_time_dim * original_in_features\n",
    "\n",
    "            # Create new 3D linear layer\n",
    "            linear3d = nn.Linear(inflated_in_features, child.out_features, bias=child.bias is not None)\n",
    "\n",
    "            # Inflate weights\n",
    "            linear2d_weights = child.weight.data # Shape [out_features, in_features_2d]\n",
    "\n",
    "            # Reshape 2D weights to [out_features, in_features_2d, 1] (add a temporal dimension)\n",
    "            inflated_weights = linear2d_weights.unsqueeze(2)\n",
    "            # Repeat along the new temporal dimension\n",
    "            inflated_weights = inflated_weights.repeat(1, 1, final_time_dim)\n",
    "            # Reshape to match the 3D linear layer's expected shape [out_features, inflated_in_features]\n",
    "            inflated_weights = inflated_weights.view(child.out_features, inflated_in_features)\n",
    "\n",
    "            linear3d.weight.data.copy_(inflated_weights)\n",
    "\n",
    "            # Copy bias if it exists\n",
    "            if child.bias is not None: \n",
    "                linear3d.bias.data.copy_(child.bias.data)\n",
    "\n",
    "            classifier_3d.add_module(name, linear3d)\n",
    "\n",
    "        else:\n",
    "            # print(f\"Warning: Skipping unhandled layer type in classifier: {name} ({type(child)})\")\n",
    "            pass # DenseNet's classifier is typically just a Linear layer\n",
    "\n",
    "\n",
    "\n",
    "    class InflatedDenseNetModel(nn.Module):\n",
    "        def __init__(self, features_3d, classifier_3d, final_time_dim, final_layer_nb):\n",
    "            super().__init__()\n",
    "            self.features = features_3d\n",
    "            self.classifier = classifier_3d\n",
    "            self.final_time_dim = final_time_dim\n",
    "            self.final_layer_nb = final_layer_nb # This is the number of channels before global pooling\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            # Apply ReLU after features (matches typical DenseNet flow before classification)\n",
    "            x = nn.functional.relu(x)\n",
    "            # Global spatial average pooling. Kernel size matches expected output spatial dims.\n",
    "            # Assuming 7x7 spatial dims before classification for DenseNet121 on ImageNet size inputs\n",
    "            spatial_kernel_h = x.shape[-2]\n",
    "            spatial_kernel_w = x.shape[-1]\n",
    "            x = nn.functional.avg_pool3d(x, kernel_size=(1, spatial_kernel_h, spatial_kernel_w))\n",
    "            # Flatten for classifier\n",
    "            # Original shape: [batch, channels, depth, 1, 1] after spatial pooling\n",
    "            # Permute to [batch, depth, channels, 1, 1]\n",
    "            x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "            # View to [batch, depth * channels]\n",
    "            #x = x.view(-1, self.final_time_dim * self.final_layer_nb)\n",
    "            x = x.view(x.size(0), -1)  # [B, N]\n",
    "\n",
    "            # Pass through classifier\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "    # Get the number of channels before the original 2D classifier\n",
    "    final_layer_nb = densenet2d.classifier.in_features\n",
    "\n",
    "    return InflatedDenseNetModel(features_3d, classifier_3d, final_time_dim, final_layer_nb)\n",
    "\n",
    "\n",
    "# --- Testing the Inflation ---\n",
    "\n",
    "import warnings\n",
    "\n",
    "# a. Take a 2D pretrained DenseNet121 model\n",
    "model_2d = models.densenet121(pretrained=True)\n",
    "print(\"Loaded 2D DenseNet121 model.\")\n",
    "\n",
    "\n",
    "input_frame_nb = 512\n",
    "# Temporal kernel depth for inflating 3x3 convs\n",
    "conv_k_depth = 3\n",
    "# Temporal stride for pooling in transition blocks\n",
    "pool_t_stride = 2\n",
    "\n",
    "# Inflate the model\n",
    "i3d_densenet_model = inflate_densenet121(\n",
    "    model_2d,\n",
    "    frame_nb=input_frame_nb,\n",
    "    conv_kernel_depth=conv_k_depth,\n",
    "    temporal_pool_stride=pool_t_stride\n",
    ")\n",
    "\n",
    "print(f\"\\nInflated 3D DenseNet model created for {input_frame_nb} frames.\")\n",
    "# Print the structure of the inflated model (optional, can be long)\n",
    "# print(i3d_densenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellUniqueIdByVincent": "cc955",
    "execution": {
     "iopub.execute_input": "2025-06-09T08:15:04.353678Z",
     "iopub.status.busy": "2025-06-09T08:15:04.353264Z",
     "iopub.status.idle": "2025-06-09T08:17:02.754073Z",
     "shell.execute_reply": "2025-06-09T08:17:02.753278Z",
     "shell.execute_reply.started": "2025-06-09T08:15:04.353653Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dummy input shape: torch.Size([2, 1, 512, 224, 224])\n",
      "\n",
      "Forward pass successful!\n",
      "Output shape: torch.Size([2, 65536])\n",
      "Expected output shape: (2, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy 3D input tensor\n",
    "# Shape: [batch_size, channels, depth, height, width]\n",
    "batch_size = 2\n",
    "channels = 1\n",
    "input_height = 224\n",
    "input_width = 224\n",
    "\n",
    "dummy_input_3d = torch.randn(batch_size, channels, input_frame_nb, input_height, input_width)\n",
    "print(\"\\nDummy input shape:\", dummy_input_3d.shape)\n",
    "\n",
    "\n",
    "# Perform a forward pass with the dummy input\n",
    "try:\n",
    "    i3d_densenet_model.eval() # Set model to evaluation mode (disables dropout, uses running stats for BN)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        output = i3d_densenet_model(dummy_input_3d)\n",
    "\n",
    "    print(\"\\nForward pass successful!\")\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # The output shape should be [batch_size, num_classes]\n",
    "    # For DenseNet121 pre-trained on ImageNet, num_classes is 1000\n",
    "    print(\"Expected output shape:\", (batch_size, 1000))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nForward pass failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "9ac65"
   },
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellUniqueIdByVincent": "0df4f",
    "execution": {
     "iopub.execute_input": "2025-06-09T11:18:39.571257Z",
     "iopub.status.busy": "2025-06-09T11:18:39.570977Z",
     "iopub.status.idle": "2025-06-09T11:18:39.578521Z",
     "shell.execute_reply": "2025-06-09T11:18:39.577846Z",
     "shell.execute_reply.started": "2025-06-09T11:18:39.571236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def global_avg_pool_3d(x):\n",
    "    return F.adaptive_avg_pool3d(x, 1).view(x.size(0), -1)\n",
    "\n",
    "def extract_region_features(model_3d, volume_dict):\n",
    "    \"\"\"\n",
    "    Extract features from 3 layers for each region in a 3D CT scan.\n",
    "\n",
    "    Args:\n",
    "        model_3d: Inflated DenseNet121 model (Conv3D based).\n",
    "        volume_dict: Dict with keys 'Tibia', 'Femur', 'Background', each with a 3D volume tensor [1, C, D, H, W].\n",
    "\n",
    "    Returns:\n",
    "        Dictionary:\n",
    "        {\n",
    "            \"Tibia\": {\"fifth_last\": ..., \"third_last\": ..., \"last\": ...},\n",
    "            \"Femur\": {...},\n",
    "            \"Background\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    model_3d.eval()\n",
    "    output_dict = {}\n",
    "\n",
    "    # Prepare hooks\n",
    "    feature_maps = {}\n",
    "    def make_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            feature_maps[name] = output\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for the 3 layers\n",
    "    hooks = []\n",
    "    hooks.append(model_3d.features.denseblock2.register_forward_hook(make_hook('fifth_last')))\n",
    "    hooks.append(model_3d.features.denseblock3.register_forward_hook(make_hook('third_last')))\n",
    "    hooks.append(model_3d.features.denseblock4.register_forward_hook(make_hook('last')))\n",
    "\n",
    "    # Process each region separately\n",
    "    with torch.no_grad():\n",
    "        for region_name, volume in volume_dict.items():\n",
    "            feature_maps.clear()\n",
    "            _ = model_3d(volume)  # Forward pass\n",
    "            output_dict[region_name] = {\n",
    "                name: global_avg_pool_3d(feat) for name, feat in feature_maps.items()\n",
    "            }\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellUniqueIdByVincent": "22404",
    "execution": {
     "iopub.execute_input": "2025-06-09T11:18:39.830214Z",
     "iopub.status.busy": "2025-06-09T11:18:39.829433Z",
     "iopub.status.idle": "2025-06-09T11:25:07.255316Z",
     "shell.execute_reply": "2025-06-09T11:25:07.254562Z",
     "shell.execute_reply.started": "2025-06-09T11:18:39.830186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tibia Features:\n",
      "  fifth_last: torch.Size([1, 512])\n",
      "  third_last: torch.Size([1, 1024])\n",
      "  last: torch.Size([1, 1024])\n",
      "\n",
      "Femur Features:\n",
      "  fifth_last: torch.Size([1, 512])\n",
      "  third_last: torch.Size([1, 1024])\n",
      "  last: torch.Size([1, 1024])\n",
      "\n",
      "Background Features:\n",
      "  fifth_last: torch.Size([1, 512])\n",
      "  third_last: torch.Size([1, 1024])\n",
      "  last: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "def prepare_volumes( tibia_mask,femur_mask,background_mask):\n",
    "    \n",
    "    tibia_volume=load_nifti_to_tensor(tibia_mask)\n",
    "    femur_volume=load_nifti_to_tensor(femur_mask)\n",
    "    background_volume=load_nifti_to_tensor(background_mask)\n",
    "    \n",
    "    tibia_volume=tibia_volume.unsqueeze(0).unsqueeze(0)\n",
    "    femur_volume=femur_volume.unsqueeze(0).unsqueeze(0)\n",
    "    background_volume=background_volume.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return {\n",
    "        \"Tibia\": tibia_volume,\n",
    "        \"Femur\": femur_volume,\n",
    "        \"Background\": background_volume,\n",
    "    }\n",
    "\n",
    "def load_nifti_to_tensor(path):\n",
    "    \"\"\"\n",
    "    Load .nii or .nii.gz file into a PyTorch tensor of shape [D, H, W]\n",
    "    \"\"\"\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata()  # Returns NumPy array, shape: [D, H, W] or [H, W, D]\n",
    "    data = torch.tensor(data).permute(2, 0, 1)  # Rearrange to [D, H, W]\n",
    "    return data.float()\n",
    "\n",
    "\n",
    "tibia_mask_path='/kaggle/input/segmented-regions/tibia.nii'\n",
    "femur_mask_path='/kaggle/input/segmented-regions/femur.nii'\n",
    "background_mask_path='/kaggle/input/segmented-regions/background.nii'\n",
    "\n",
    "# Example input: [1, 3, 16, 128, 128] tensors for each region\n",
    "volume_dict = prepare_volumes(tibia_mask_path,femur_mask_path,background_mask_path)\n",
    "\n",
    "features = extract_region_features(i3d_densenet_model, volume_dict)\n",
    "\n",
    "# Print feature dimensions\n",
    "for region, feats in features.items():\n",
    "    print(f\"\\n{region} Features:\")\n",
    "    for layer, vec in feats.items():\n",
    "        print(f\"  {layer}: {vec.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellUniqueIdByVincent": "67a1f",
    "execution": {
     "iopub.execute_input": "2025-06-09T11:50:28.223666Z",
     "iopub.status.busy": "2025-06-09T11:50:28.223357Z",
     "iopub.status.idle": "2025-06-09T11:50:28.242909Z",
     "shell.execute_reply": "2025-06-09T11:50:28.242089Z",
     "shell.execute_reply.started": "2025-06-09T11:50:28.223645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cosine similarity saved to region_cosine_similarities.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Assume this is already defined\n",
    "# features = {...}\n",
    "\n",
    "regions = ['Tibia', 'Femur', 'Background']\n",
    "layers = ['fifth_last', 'third_last', 'last']\n",
    "\n",
    "# All pairs of regions\n",
    "region_pairs = [\n",
    "    ('Tibia', 'Femur'),\n",
    "    ('Tibia', 'Background'),\n",
    "    ('Femur', 'Background')\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for layer in layers:\n",
    "    for r1, r2 in region_pairs:\n",
    "        v1 = features[r1][layer]\n",
    "        v2 = features[r2][layer]\n",
    "        \n",
    "        # Flatten if needed\n",
    "        v1 = v1.view(-1)\n",
    "        v2 = v2.view(-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = F.cosine_similarity(v1, v2, dim=0).item()\n",
    "        \n",
    "        # Key format: \"tibia_femur_last\"\n",
    "        key = f\"{r1}_{r2}_{layer}\"\n",
    "        results[key] = similarity\n",
    "\n",
    "# Convert to DataFrame (1 row, many columns)\n",
    "df = pd.DataFrame([results])\n",
    "df.to_csv(\"region_cosine_similarities.csv\", index=False)\n",
    "\n",
    "print(\" Cosine similarity saved to region_cosine_similarities.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "22454",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellUniqueIdByVincent": "ad3c3",
    "execution": {
     "iopub.execute_input": "2025-06-09T12:02:28.372815Z",
     "iopub.status.busy": "2025-06-09T12:02:28.372520Z",
     "iopub.status.idle": "2025-06-09T12:04:37.764882Z",
     "shell.execute_reply": "2025-06-09T12:04:37.763942Z",
     "shell.execute_reply.started": "2025-06-09T12:02:28.372791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1702159018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mshow_feature_map_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tibia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tibia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35/1702159018.py\u001b[0m in \u001b[0;36mshow_feature_map_slices\u001b[0;34m(feature_tensor, region, layer, num_channels)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mfeature_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -> [C, D, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mmid_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mselected_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNet3DFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_3d):\n",
    "        super().__init__()\n",
    "        self.model = model_3d\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = {}\n",
    "        \n",
    "        x = self.model.features.conv0(x)\n",
    "        x = self.model.features.norm0(x)\n",
    "        x = self.model.features.relu0(x)\n",
    "        x = self.model.features.pool0(x)\n",
    "\n",
    "        x = self.model.features.denseblock1(x)\n",
    "        x = self.model.features.transition1(x)\n",
    "        fmap['fifth_last'] = x.clone()\n",
    "\n",
    "        x = self.model.features.denseblock2(x)\n",
    "        x = self.model.features.transition2(x)\n",
    "        fmap['third_last'] = x.clone()\n",
    "\n",
    "        x = self.model.features.denseblock3(x)\n",
    "        x = self.model.features.transition3(x)\n",
    "        fmap['last'] = x.clone()\n",
    "\n",
    "        return fmap  # Return full spatial feature maps\n",
    "# Suppose you already have an input volume `x` with shape [1, C, D, H, W]\n",
    "x=volume_dict['Tibia']\n",
    "\n",
    "#x = x.to(device)\n",
    "\n",
    "# Wrap your model\n",
    "wrapped_model = DenseNet3DFeatureExtractor(i3d_densenet_model)\n",
    "wrapped_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    feature_maps = wrapped_model(x)\n",
    "\n",
    "# Now feature_maps['last'], ['third_last'], etc. are 5D: [1, C, D, H, W]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellUniqueIdByVincent": "3e730",
    "execution": {
     "iopub.execute_input": "2025-06-09T12:09:00.998212Z",
     "iopub.status.busy": "2025-06-09T12:09:00.997732Z",
     "iopub.status.idle": "2025-06-09T12:09:01.445111Z",
     "shell.execute_reply": "2025-06-09T12:09:01.444381Z",
     "shell.execute_reply.started": "2025-06-09T12:09:00.998188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_tensor(t):\n",
    "    t = t - t.min()\n",
    "    t = t / (t.max() + 1e-8)\n",
    "    return t\n",
    "def show_feature_map_slices(feature_tensor, region=\"tibia\", layer=\"last\", num_channels=4):\n",
    "    # feature_tensor: [1, C, D, H, W]\n",
    "    feature_tensor = feature_tensor.squeeze(0).detach().cpu()  # -> [C, D, H, W]\n",
    "    \n",
    "    C, D, H, W = feature_tensor.shape\n",
    "    mid_depth = D // 2\n",
    "    selected_channels = np.linspace(0, C-1, num_channels, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_channels, figsize=(4 * num_channels, 4))\n",
    "    fig.suptitle(f\"{region.upper()} - {layer} layer: feature map slices at depth={mid_depth}\", fontsize=16)\n",
    "\n",
    "    for i, ch in enumerate(selected_channels):\n",
    "        fmap = feature_tensor[ch, mid_depth, :, :]  # Slice at middle depth\n",
    "        fmap = normalize_tensor(fmap)\n",
    "        axes[i].imshow(fmap, cmap='viridis')\n",
    "        axes[i].set_title(f\"Channel {ch}\")\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Feature_maps_fifth_last_tibia')\n",
    "    plt.show()\n",
    "show_feature_map_slices(feature_maps['fifth_last'], region='tibia', layer='fifth_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "966e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index_"
   ]
  }
 ],
 "metadata": {
  "vincent": {
   "sessionId": "04adde7c228161df45d147ff_2025-06-12T19-38-08-153Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
